{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pkgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is training notebook only. Inference ain't included in . \n",
    "Anybody who wants to use this notebook for inference purposes is most welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchinfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import zarr, copick\n",
    "from tqdm import tqdm\n",
    "import napari\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from copick_utils.segmentation import segmentation_from_picks\n",
    "import copick_utils.writers.write as write\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "import concurrent.futures\n",
    "import optuna, optunahub\n",
    "from optuna.visualization import plot_slice, plot_param_importances\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.cache_size_limit = 64\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_number = '20250202_05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/'\n",
    "output_path = path + 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a copick project\n",
    "\n",
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 5,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/output/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/train/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = path + \"output/copick.config\"\n",
    "output_overlay = path + \"output/overlay\"\n",
    "\n",
    "with open(copick_config_path, \"w\") as f:\n",
    "    f.write(config_blob)\n",
    "    \n",
    "# Update the overlay\n",
    "# Define source and destination directories\n",
    "source_dir = path + 'train/overlay'\n",
    "destination_dir = path + 'output/overlay'\n",
    "\n",
    "# Walk through the source directory\n",
    "def some_function():\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        # Create corresponding subdirectories in the destination\n",
    "        relative_path = os.path.relpath(root, source_dir)\n",
    "        target_dir = os.path.join(destination_dir, relative_path)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy and rename each file\n",
    "        for file in files:\n",
    "            if file.startswith(\"curation_0_\"):\n",
    "                new_filename = file\n",
    "            else:\n",
    "                new_filename = f\"curation_0_{file}\"\n",
    "                \n",
    "            \n",
    "            # Define full paths for the source and destination files\n",
    "            source_file = os.path.join(root, file)\n",
    "            destination_file = os.path.join(target_dir, new_filename)\n",
    "            \n",
    "            # Copy the file with the new name\n",
    "            shutil.copy2(source_file, destination_file)\n",
    "            print(f\"Copied {source_file} to {destination_file}\")\n",
    "\n",
    "some_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = copick.from_file(copick_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10\n",
    "#tomo_type = \"denoised\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate multi-class segmentation masks from picks, and saved them to the copick overlay directory (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_function():\n",
    "    target_objects = defaultdict(dict)\n",
    "    for object in root.pickable_objects:\n",
    "        if object.is_particle:\n",
    "            target_objects[object.name]['label'] = object.label\n",
    "            target_objects[object.name]['radius'] = object.radius\n",
    "\n",
    "\n",
    "    for run in tqdm(root.runs):\n",
    "        tomo = run.get_voxel_spacing(voxel_size)\n",
    "        for tomogram in tomo.tomograms:\n",
    "            tomo_type = tomogram.tomo_type\n",
    "            if tomo_type != 'denoised':\n",
    "                continue\n",
    "            image = tomogram.numpy()\n",
    "            target = np.zeros(image.shape, dtype=np.uint8)\n",
    "            for pickable_object in root.pickable_objects:\n",
    "                pick = run.get_picks(object_name=pickable_object.name, user_id='curation')\n",
    "                if len(pick):\n",
    "                    target = segmentation_from_picks.from_picks(pick[0],\n",
    "                                                                target,\n",
    "                                                                target_objects[pickable_object.name]['radius'],\n",
    "                                                                target_objects[pickable_object.name]['label']\n",
    "                                                               )\n",
    "            write.segmentation(run, target, copick_user_name, name=copick_segmentation_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do this once\n",
    "generate_masks = True\n",
    "\n",
    "if generate_masks:\n",
    "    generate_masks_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_experiment_folders_path = '/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/' + 'train/overlay/ExperimentRuns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = {\n",
    "    'apo-ferritin': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_df(experiment):\n",
    "    labels_dict = {}\n",
    "    \n",
    "    particle_types_dict = {}\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/apo-ferritin.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['apo-ferritin'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/beta-galactosidase.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['beta-galactosidase'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/ribosome.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['ribosome'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/thyroglobulin.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['thyroglobulin'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/virus-like-particle.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['virus-like-particle'] = loaded_json\n",
    "    \n",
    "    labels_dict[experiment] = particle_types_dict\n",
    "\n",
    "    experiment_list = []\n",
    "    particle_type_list = []\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    r_list = []\n",
    "    class_id_list = []\n",
    "    #print(experiment)\n",
    "    #print(len(labels_dict[experiment]['apo-ferritin']['points']))\n",
    "    #print(type(labels_dict[experiment]['apo-ferritin']['points']))\n",
    "    #print(labels_dict[experiment]['apo-ferritin']['points'][0])\n",
    "\n",
    "    for key in labels_dict[experiment].keys():\n",
    "        #print(labels_dict[experiment][key])\n",
    "        #print(labels_dict[experiment][key]['pickable_object_name'])\n",
    "        for i in range(len(labels_dict[experiment][key]['points'])):\n",
    "            experiment_list.append(labels_dict[experiment][key]['run_name'])\n",
    "            particle_type_list.append(labels_dict[experiment][key]['pickable_object_name'])\n",
    "            x_list.append(labels_dict[experiment][key]['points'][i]['location']['x']/10.012444537618887)\n",
    "            y_list.append(labels_dict[experiment][key]['points'][i]['location']['y']/10.012444196428572)\n",
    "            z_list.append(labels_dict[experiment][key]['points'][i]['location']['z']/10.012444196428572)\n",
    "            r_list.append(particle_radius[key]/10)\n",
    "            class_id_list.append(class_ids[key])\n",
    "\n",
    "    labels_df = pd.DataFrame({'experiment':experiment_list, 'particle_type':particle_type_list, 'x':x_list, 'y':y_list, 'z':z_list, 'radius':r_list, 'label':class_id_list})\n",
    "    \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enzyme radii\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 60 / 10,\n",
    "    'beta-galactosidase': 90 / 10,\n",
    "    'ribosome': 150 / 10,\n",
    "    'thyroglobulin': 130 / 10,\n",
    "    'virus-like-particle': 135 / 10,\n",
    "}\n",
    "\n",
    "# Map enzyme names to numerical class labels (for segmentation)\n",
    "enzyme_class_mapping = {\n",
    "    'apo-ferritin': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_heatmaps(image_shape, locations_df, sigma_scale=1):\n",
    "    \"\"\"\n",
    "    Creates a 5-channel Gaussian heatmap volume (one channel per enzyme class).\n",
    "    Each channel contains Gaussian blobs centered at enzyme locations of that class.\n",
    "    \n",
    "    Args:\n",
    "        image_shape: (z, y, x) tuple - shape of 3D volume\n",
    "        locations_df: DataFrame with columns ['z', 'y', 'x', 'particle_type']\n",
    "        sigma_scale: Ratio of sigma to enzyme radius (controls Gaussian spread)\n",
    "    \n",
    "    Returns:\n",
    "        heatmaps: np.ndarray with shape (5, z, y, x)\n",
    "    \"\"\"\n",
    "    num_classes = 5\n",
    "    heatmaps = np.zeros((num_classes,) + image_shape, dtype=np.float32)\n",
    "\n",
    "    centers = locations_df[['z', 'y', 'x']].values\n",
    "    enzyme_names = locations_df['particle_type'].values\n",
    "\n",
    "    for center, enzyme_name in zip(centers, enzyme_names):\n",
    "        if enzyme_name not in particle_radius:\n",
    "            continue\n",
    "\n",
    "        # Get enzyme properties\n",
    "        radius = particle_radius[enzyme_name]\n",
    "        class_label = enzyme_class_mapping[enzyme_name]  # Should map to 1-5\n",
    "        channel_idx = class_label - 1  # Convert to 0-based index\n",
    "\n",
    "        # Calculate Gaussian parameters\n",
    "        sigma = radius * sigma_scale\n",
    "        truncate = 3  # Number of sigmas to consider for bounding box\n",
    "\n",
    "        # Bounding box calculations (3σ truncation)\n",
    "        z_min = max(0, int(center[0] - sigma * truncate))\n",
    "        z_max = min(image_shape[0], int(center[0] + sigma * truncate) + 1)\n",
    "        y_min = max(0, int(center[1] - sigma * truncate))\n",
    "        y_max = min(image_shape[1], int(center[1] + sigma * truncate) + 1)\n",
    "        x_min = max(0, int(center[2] - sigma * truncate))\n",
    "        x_max = min(image_shape[2], int(center[2] + sigma * truncate) + 1)\n",
    "\n",
    "        if z_min >= z_max or y_min >= y_max or x_min >= x_max:\n",
    "            continue\n",
    "\n",
    "        # Create local grid\n",
    "        zz, yy, xx = np.meshgrid(\n",
    "            np.arange(z_min, z_max),\n",
    "            np.arange(y_min, y_max),\n",
    "            np.arange(x_min, x_max),\n",
    "            indexing='ij'\n",
    "        )\n",
    "\n",
    "        # Compute squared distance from center\n",
    "        dx = xx - center[2]  # x-coordinate difference\n",
    "        dy = yy - center[1]  # y-coordinate difference\n",
    "        dz = zz - center[0]  # z-coordinate difference\n",
    "        dist_sq = dx**2 + dy**2 + dz**2\n",
    "\n",
    "        # Compute Gaussian values\n",
    "        gaussian = np.exp(-dist_sq / (2 * sigma**2))\n",
    "        \n",
    "        # Add to the corresponding channel (using np.maximum to handle overlaps)\n",
    "        heatmaps[channel_idx, z_min:z_max, y_min:y_max, x_min:x_max] = np.maximum(\n",
    "            heatmaps[channel_idx, z_min:z_max, y_min:y_max, x_min:x_max],\n",
    "            gaussian\n",
    "        )\n",
    "\n",
    "    return heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    images_list = []\n",
    "    heatmaps_list = []\n",
    "    segmentations_list = []\n",
    "    label_dfs_list = []\n",
    "    \n",
    "    for run in tqdm(root.runs):\n",
    "        tomo = run.get_voxel_spacing(voxel_size)#.get_tomograms(tomo_type)[0].numpy()\n",
    "        labels_df = create_labels_df(run.name)\n",
    "        for tomogram in tomo.tomograms:\n",
    "            tomo_type = tomogram.tomo_type\n",
    "            if tomo_type != 'denoised':\n",
    "                continue\n",
    "            image = tomogram.numpy()\n",
    "            segmentation = run.get_segmentations(name=copick_segmentation_name, user_id=copick_user_name, voxel_size=voxel_size, is_multilabel=True)[0].numpy()\n",
    "            segmentations_list.append(segmentation)\n",
    "            images_list.append(image)\n",
    "\n",
    "            heatmap = create_gaussian_heatmaps(image.shape, labels_df)\n",
    "            heatmaps_list.append(heatmap)\n",
    "            \n",
    "            label_dfs_list.append(labels_df)\n",
    "\n",
    "    images_array = np.array(images_list)\n",
    "    segmentations_array = np.array(segmentations_list)\n",
    "    heatmaps_array = np.array(heatmaps_list)\n",
    "\n",
    "    images_array = np.expand_dims(images_array, axis=1)\n",
    "    segmentations_array = np.expand_dims(segmentations_array, axis=1)\n",
    "\n",
    "    return images_array, heatmaps_array, segmentations_array, label_dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array, heatmaps_array, segmentations_array, label_dfs_list = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dfs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names_list = []\n",
    "for i in range(len(label_dfs_list)):\n",
    "    exp_name = label_dfs_list[i]['experiment'][0]\n",
    "    exp_names_list.append(exp_name)\n",
    "exp_names_sr = pd.Series(exp_names_list)\n",
    "unique_exp_names = exp_names_sr.unique()\n",
    "unique_exp_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the tomogram and gaussian heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 7, figsize=(28, 4))  # Adjust figsize to control spacing\n",
    "\n",
    "# Image 1\n",
    "axes[0].imshow(images_array[0][0][100])\n",
    "axes[0].set_title('Tomogram')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image 2\n",
    "axes[1].imshow(segmentations_array[0][0][100])\n",
    "axes[1].set_title('Segmentation mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Image 3\n",
    "axes[2].imshow(heatmaps_array[0][0][100])\n",
    "axes[2].set_title('Enzyme 1 apo-ferritin heatmap')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Image 4\n",
    "axes[3].imshow(heatmaps_array[0][1][100])\n",
    "axes[3].set_title('Enzyme 2 beta-galactosidase heatmap')\n",
    "axes[3].axis('off')\n",
    "\n",
    "# Image 5\n",
    "axes[4].imshow(heatmaps_array[0][2][100])\n",
    "axes[4].set_title('Enzyme 3 ribosome heatmap')\n",
    "axes[4].axis('off')\n",
    "\n",
    "# Image 6\n",
    "axes[5].imshow(heatmaps_array[0][3][100])\n",
    "axes[5].set_title('Enzyme 4 thyroglobulin heatmap')\n",
    "axes[5].axis('off')\n",
    "\n",
    "# Image 7\n",
    "axes[6].imshow(heatmaps_array[0][4][100])\n",
    "axes[6].set_title('Enzyme 5 virus-like-particle heatmap')\n",
    "axes[6].axis('off')\n",
    "\n",
    "# Reduce spacing\n",
    "plt.subplots_adjust(wspace=0.05)  # Reduce horizontal space between images\n",
    "plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)  # Further minimize padding\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_path = '../../czii_downloaded_data/simulated_training_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulated_experiments_list = [f for f in os.listdir(simulated_data_path) if 'TS_' in f]\n",
    "simulated_experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_info = {\n",
    "    \"apo-ferritin\": {\"label\": 1, \"radius\": 60},\n",
    "    \"beta-galactosidase\": {\"label\": 2, \"radius\": 90},\n",
    "    \"ribosome\": {\"label\": 3, \"radius\": 150},\n",
    "    \"thyroglobulin\": {\"label\": 4, \"radius\": 130},\n",
    "    \"virus-like-particle\": {\"label\": 5, \"radius\": 135}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_name_dict = {\n",
    "    'ferritin':'apo-ferritin',\n",
    "    'galacto':'beta-galactosidase',\n",
    "    'ribosome':'ribosome',\n",
    "    'thyro':'thyroglobulin',\n",
    "    'vlp':'virus-like-particle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(experiment):\n",
    "    \"\"\"Process a single experiment in parallel\"\"\"\n",
    "    # Load tomogram data\n",
    "    tomogram_path = f'{simulated_data_path}{experiment}/Reconstructions/VoxelSpacing10.000/Tomograms/100/{experiment}.zarr'\n",
    "    tomogram = zarr.open(tomogram_path, mode='r')[0].__array__()\n",
    "    \n",
    "    location_dfs_list = []\n",
    "    annotations_base = f'{simulated_data_path}{experiment}/Reconstructions/VoxelSpacing10.000/Annotations/'\n",
    "    \n",
    "    # Process location files\n",
    "    for location_folder in os.listdir(annotations_base):\n",
    "        location_dir = os.path.join(annotations_base, location_folder)\n",
    "        for location_file in os.listdir(location_dir):\n",
    "            if 'orientedpoint.ndjson' not in location_file:\n",
    "                continue\n",
    "                \n",
    "            # Determine particle type\n",
    "            particle_type = None\n",
    "            for key in particle_name_dict.keys():\n",
    "                if key in location_file:\n",
    "                    particle_type = particle_name_dict[key]\n",
    "                    break\n",
    "            if not particle_type:\n",
    "                continue\n",
    "                \n",
    "            # Process NDJSON file\n",
    "            label = particle_info[particle_type]['label']\n",
    "            radius = particle_info[particle_type]['radius']\n",
    "            file_path = os.path.join(location_dir, location_file)\n",
    "            \n",
    "            location_df = pd.read_json(file_path, lines=True)\n",
    "            location_df['x'] = location_df['location'].map(lambda x: x['x'])\n",
    "            location_df['y'] = location_df['location'].map(lambda x: x['y'])\n",
    "            location_df['z'] = location_df['location'].map(lambda x: x['z'])\n",
    "            location_df['label'] = label\n",
    "            location_df['radius'] = radius / 10.012444\n",
    "            location_df['particle_type'] = particle_type\n",
    "            \n",
    "            location_dfs_list.append(location_df)\n",
    "    \n",
    "    # Create mask and return result\n",
    "    if location_dfs_list:\n",
    "        all_particle_locations_df = pd.concat(location_dfs_list, ignore_index=True)\n",
    "        mask_image = create_gaussian_heatmaps(tomogram.shape, all_particle_locations_df)\n",
    "        all_particle_locations_df['experiment'] = experiment\n",
    "        label_df = all_particle_locations_df[['experiment', 'particle_type', 'x', 'y', 'z', 'radius', 'label']]\n",
    "        return {\"tomo_type\": 'Unknown', \"image\": tomogram, \"label\": mask_image, \"label_df\": label_df}\n",
    "    return None\n",
    "\n",
    "def append_simulation_data():\n",
    "    global data_dicts\n",
    "    max_workers = os.cpu_count() // 2  # Use half the available cores to prevent memory issues\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all experiments for processing\n",
    "        futures = [executor.submit(process_experiment, exp) for exp in simulated_experiments_list]\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                         total=len(simulated_experiments_list),\n",
    "                         desc=\"Processing experiments\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                data_dicts.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append_simulation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, EnsureChannelFirst, NormalizeIntensity, Orientation, RandGaussianNoise, RandRotate90, RandFlip, RandSpatialCrop, RandAdjustContrast, RandCropByLabelClasses\n",
    "\n",
    "num_classes = 5\n",
    "my_num_samples = 16\n",
    "loader_workers = 8\n",
    "cube_size = 160\n",
    "batch_size = 1\n",
    "\n",
    "# Non-random transforms to be cached\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirst(channel_dim=0),\n",
    "    Orientation(axcodes=\"RAS\"),\n",
    "])\n",
    "\n",
    "images_only_transforms = Compose([\n",
    "    NormalizeIntensity(),\n",
    "    #RandGaussianNoise(prob=0.2, std=0.01),\n",
    "    #RandAdjustContrast(prob=0.5, gamma=(0.7, 1.3)),  # Random contrast change\n",
    "])\n",
    "\n",
    "random_transforms = Compose([\n",
    "    RandRotate90(prob=0.5, spatial_axes=(1, 2)),\n",
    "    RandFlip(prob=0.5, spatial_axis=0),\n",
    "    RandSpatialCrop(roi_size=(cube_size, cube_size, cube_size), random_center=True),\n",
    "])\n",
    "\n",
    "val_images_only_transforms = Compose([\n",
    "    NormalizeIntensity(),\n",
    "])\n",
    "\n",
    "class EnzymeDataset(Dataset):\n",
    "    def __init__(self, images, heatmaps, non_random_transform=None, image_only_transform=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: np.ndarray/tensor of shape (num_samples, 1, D, H, W)\n",
    "            heatmaps: np.ndarray/tensor of shape (num_samples, 5, D, H, W)\n",
    "            transform: Callable transform to apply to both image and heatmap\n",
    "        \"\"\"\n",
    "        self.images = torch.as_tensor(images, dtype=torch.float32)\n",
    "        self.heatmaps = torch.as_tensor(heatmaps, dtype=torch.float32)\n",
    "        self.non_random_transform = non_random_transform\n",
    "        self.image_only_transform = image_only_transform\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Shape: (1, D, H, W)\n",
    "        heatmap = self.heatmaps[idx]  # Shape: (5, D, H, W)\n",
    "\n",
    "        if self.non_random_transform:\n",
    "            combined = torch.cat([image, heatmap], dim=0)\n",
    "            combined = self.non_random_transform(combined)\n",
    "            image = combined[0:1]\n",
    "            heatmap = combined[1:]\n",
    "\n",
    "        if self.image_only_transform:\n",
    "            image = self.image_only_transform(image)\n",
    "            \n",
    "\n",
    "        if self.transform:\n",
    "            # Apply the same spatial transform to both image and heatmap\n",
    "            # Concatenate to process together (channel dimension becomes 1+5=6)\n",
    "            combined = torch.cat([image, heatmap], dim=0)  # (6, D, H, W)\n",
    "            combined = self.transform(combined)\n",
    "            \n",
    "            # Split back into image and heatmap\n",
    "            image = combined[0:1]  # (1, D, H, W)\n",
    "            heatmap = combined[1:]  # (5, D, H, W)\n",
    "\n",
    "        return image, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_array = images_array[2:]\n",
    "train_images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_heatmaps_array = heatmaps_array[2:]\n",
    "train_heatmaps_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_array = images_array[:2]\n",
    "val_images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_heatmaps_array = heatmaps_array[:2]\n",
    "val_heatmaps_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = EnzymeDataset(train_images_array, train_heatmaps_array, non_random_transforms, images_only_transforms, random_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=loader_workers, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = EnzymeDataset(val_images_array, val_heatmaps_array, non_random_transforms, val_images_only_transforms, random_transforms)\n",
    "val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=loader_workers, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dfs = label_dfs_list[2:]\n",
    "len(train_label_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_dfs = label_dfs_list[:2]\n",
    "len(val_label_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Derived from:\n",
    "https://github.com/cellcanvas/album-catalog/blob/main/solutions/copick/compare-picks/solution.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_metrics(reference_points, reference_radius, candidate_points):\n",
    "    num_reference_particles = len(reference_points)\n",
    "    num_candidate_particles = len(candidate_points)\n",
    "\n",
    "    if len(reference_points) == 0:\n",
    "        return 0, num_candidate_particles, 0\n",
    "\n",
    "    if len(candidate_points) == 0:\n",
    "        return 0, 0, num_reference_particles\n",
    "\n",
    "    ref_tree = KDTree(reference_points)\n",
    "    candidate_tree = KDTree(candidate_points)\n",
    "    raw_matches = candidate_tree.query_ball_tree(ref_tree, r=reference_radius)\n",
    "    matches_within_threshold = []\n",
    "    for match in raw_matches:\n",
    "        matches_within_threshold.extend(match)\n",
    "    # Prevent submitting multiple matches per particle.\n",
    "    # This won't be be strictly correct in the (extremely rare) case where true particles\n",
    "    # are very close to each other.\n",
    "    matches_within_threshold = set(matches_within_threshold)\n",
    "    tp = int(len(matches_within_threshold))\n",
    "    fp = int(num_candidate_particles - tp)\n",
    "    fn = int(num_reference_particles - tp)\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        row_id_column_name: str,\n",
    "        distance_multiplier: float,\n",
    "        beta: int) -> float:\n",
    "    '''\n",
    "    F_beta\n",
    "      - a true positive occurs when\n",
    "         - (a) the predicted location is within a threshold of the particle radius, and\n",
    "         - (b) the correct `particle_type` is specified\n",
    "      - raw results (TP, FP, FN) are aggregated across all experiments for each particle type\n",
    "      - f_beta is calculated for each particle type\n",
    "      - individual f_beta scores are weighted by particle type for final score\n",
    "    '''\n",
    "\n",
    "    particle_radius = {\n",
    "        'apo-ferritin': 60,\n",
    "        'beta-amylase': 65,\n",
    "        'beta-galactosidase': 90,\n",
    "        'ribosome': 150,\n",
    "        'thyroglobulin': 130,\n",
    "        'virus-like-particle': 135,\n",
    "    }\n",
    "\n",
    "    weights = {\n",
    "        'apo-ferritin': 1,\n",
    "        'beta-amylase': 0,\n",
    "        'beta-galactosidase': 2,\n",
    "        'ribosome': 1,\n",
    "        'thyroglobulin': 2,\n",
    "        'virus-like-particle': 1,\n",
    "    }\n",
    "\n",
    "    particle_radius = {k: v * distance_multiplier for k, v in particle_radius.items()}\n",
    "\n",
    "    # Filter submission to only contain experiments found in the solution split\n",
    "    split_experiments = set(solution['experiment'].unique())\n",
    "    submission = submission.loc[submission['experiment'].isin(split_experiments)]\n",
    "\n",
    "    # Only allow known particle types\n",
    "    if not set(submission['particle_type'].unique()).issubset(set(weights.keys())):\n",
    "        raise ParticipantVisibleError('Unrecognized `particle_type`.')\n",
    "\n",
    "    assert solution.duplicated(subset=['experiment', 'x', 'y', 'z']).sum() == 0\n",
    "    assert particle_radius.keys() == weights.keys()\n",
    "\n",
    "    results = {}\n",
    "    for particle_type in solution['particle_type'].unique():\n",
    "        results[particle_type] = {\n",
    "            'total_tp': 0,\n",
    "            'total_fp': 0,\n",
    "            'total_fn': 0,\n",
    "        }\n",
    "\n",
    "    for experiment in split_experiments:\n",
    "        for particle_type in solution['particle_type'].unique():\n",
    "            reference_radius = particle_radius[particle_type]\n",
    "            select = (solution['experiment'] == experiment) & (solution['particle_type'] == particle_type)\n",
    "            reference_points = solution.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            select = (submission['experiment'] == experiment) & (submission['particle_type'] == particle_type)\n",
    "            candidate_points = submission.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            if len(reference_points) == 0:\n",
    "                reference_points = np.array([])\n",
    "                reference_radius = 1\n",
    "\n",
    "            if len(candidate_points) == 0:\n",
    "                candidate_points = np.array([])\n",
    "\n",
    "            tp, fp, fn = compute_metrics(reference_points, reference_radius, candidate_points)\n",
    "\n",
    "            results[particle_type]['total_tp'] += tp\n",
    "            results[particle_type]['total_fp'] += fp\n",
    "            results[particle_type]['total_fn'] += fn\n",
    "\n",
    "    aggregate_fbeta = 0.0\n",
    "    for particle_type, totals in results.items():\n",
    "        tp = totals['total_tp']\n",
    "        fp = totals['total_fp']\n",
    "        fn = totals['total_fn']\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        aggregate_fbeta += fbeta * weights.get(particle_type, 1.0)\n",
    "\n",
    "    if weights:\n",
    "        aggregate_fbeta = aggregate_fbeta / sum(weights.values())\n",
    "    else:\n",
    "        aggregate_fbeta = aggregate_fbeta / len(results)\n",
    "    return aggregate_fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              #2: \"beta-amylase\",\n",
    "              2: \"beta-galactosidase\", \n",
    "              3: \"ribosome\", \n",
    "              4: \"thyroglobulin\", \n",
    "              5: \"virus-like-particle\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import cc3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_3d_patches_minimal_overlap(\n",
    "    arrays: List[np.ndarray], patch_size: int, overlap: int = 48\n",
    ") -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "\n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "\n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than the smallest dimension {min(shape)}\")\n",
    "\n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "\n",
    "    # Calculate starting positions for each dimension with fixed overlap\n",
    "    x_starts = calculate_patch_starts(m, patch_size, overlap)\n",
    "    y_starts = calculate_patch_starts(n, patch_size, overlap)\n",
    "    z_starts = calculate_patch_starts(l, patch_size, overlap)\n",
    "\n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "\n",
    "    return patches, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_prediction_array(\n",
    "    patches: List[np.ndarray], \n",
    "    coordinates: List[Tuple[int, int, int]], \n",
    "    original_shape: Tuple[int, int, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstructs the full probability tensor by averaging overlapping patch contributions.\n",
    "    \n",
    "    Args:\n",
    "    - patches: List of predicted probability patches, each with shape (6, 96, 96, 96).\n",
    "    - coordinates: List of (x, y, z) positions for each patch.\n",
    "    - original_shape: Tuple (C, D, H, W) for the final tensor shape.\n",
    "    \n",
    "    Returns:\n",
    "    - A tensor of shape (6, 184, 630, 630) with averaged probabilities.\n",
    "    \"\"\"\n",
    "    num_classes = patches[0].shape[0]  # Should be 6\n",
    "\n",
    "    original_shape = tuple([num_classes] + list(original_shape))\n",
    "    \n",
    "    reconstructed = np.zeros(original_shape, dtype=np.float32)  # Stores summed probabilities\n",
    "    count = np.zeros(original_shape, dtype=np.float32)  # Stores the number of contributions\n",
    "\n",
    "    patch_size = patches[0].shape[1]  # Should be 96\n",
    "\n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            :,\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] += patch  # Sum up probabilities\n",
    "        count[\n",
    "            :,\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] += 1  # Track contributions\n",
    "\n",
    "    # Normalize to get the averaged probabilities\n",
    "    count[count == 0] = 1  # Avoid division by zero\n",
    "    \n",
    "    return torch.tensor((reconstructed / count).astype(np.float32))  # Compute the average probability per voxel\n",
    "\n",
    "\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int, overlap: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Calculates the starting indices for patch extraction, ensuring a fixed overlap.\n",
    "    \"\"\"\n",
    "    step = patch_size - overlap\n",
    "    positions = list(range(0, dimension_size - patch_size + 1, step))\n",
    "\n",
    "    # Ensure the last patch reaches the end\n",
    "    if positions[-1] + patch_size < dimension_size:\n",
    "        positions.append(dimension_size - patch_size)\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_prediction_tta(model, input_tensor):\n",
    "    probs_list = []\n",
    "    \n",
    "    data_copy0 = input_tensor.clone()\n",
    "    data_copy0 = torch.flip(data_copy0, dims=[2])\n",
    "    data_copy1 = input_tensor.clone()\n",
    "    data_copy1 = torch.flip(data_copy1, dims=[3])\n",
    "    data_copy2 = input_tensor.clone()\n",
    "    data_copy2 = torch.flip(data_copy2, dims=[4])\n",
    "    data_copy3 = input_tensor.clone()\n",
    "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
    "    \n",
    "    model_output0 = model(input_tensor)\n",
    "    model_output1 = model(data_copy0)\n",
    "    model_output1 = torch.flip(model_output1, dims=[2])\n",
    "    model_output2 = model(data_copy1)\n",
    "    model_output2 = torch.flip(model_output2, dims=[3])\n",
    "    model_output3 = model(data_copy2)\n",
    "    model_output3 = torch.flip(model_output3, dims=[4])\n",
    "    \n",
    "    probs0 = torch.softmax(model_output0[0], dim=0)\n",
    "    probs1 = torch.softmax(model_output1[0], dim=0)\n",
    "    probs2 = torch.softmax(model_output2[0], dim=0)\n",
    "    probs3 = torch.softmax(model_output3[0], dim=0)\n",
    "    \n",
    "    probs_list.append(probs0)\n",
    "    probs_list.append(probs1)\n",
    "    probs_list.append(probs2)\n",
    "    probs_list.append(probs3)\n",
    "    \n",
    "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
    "\n",
    "    return avg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_centers(volume, model, threshold=0.3):\n",
    "    \"\"\"Convert heatmaps to enzyme center coordinates.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        heatmaps = torch.sigmoid(model(volume.unsqueeze(0).to(device)))[0]\n",
    "    \n",
    "    centers = []\n",
    "    for class_idx in range(5):\n",
    "        class_heatmap = heatmaps[class_idx].cpu().numpy()\n",
    "        \n",
    "        # Find local maxima\n",
    "        peaks = peak_local_max(\n",
    "            class_heatmap, \n",
    "            min_distance=3,  # Minimum voxels between peaks\n",
    "            threshold_abs=threshold\n",
    "        )\n",
    "        \n",
    "        # Convert voxel indices to coordinates\n",
    "        for z, y, x in peaks:\n",
    "            centers.append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'z': z,\n",
    "                'class': class_idx + 1,  # Convert back to 1-5\n",
    "                'confidence': class_heatmap[z, y, x]\n",
    "            })\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirst(channel_dim=0),\n",
    "    NormalizeIntensity(),\n",
    "    Orientation(axcodes=\"RAS\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss, FocalLoss, TverskyLoss, SSIMLoss\n",
    "#from monai.metrics import DiceMetric, ConfusionMatrixMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dice_loss_function = DiceLoss(include_background=True, to_onehot_y=True, softmax=True)  # softmax=True for multiclass\n",
    "#tversky_loss_function = TverskyLoss(include_background=True, to_onehot_y=False, softmax=True)  # softmax=True for multiclass\n",
    "#dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)  # must use onehot for multiclass\n",
    "#recall_metric = ConfusionMatrixMetric(include_background=False, metric_name=\"recall\", reduction=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        loss = self.alpha * (1 - pt)**self.gamma * BCE_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_focal=0.7, lambda_ssim=0.3):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.ssim_loss = SSIMLoss(spatial_dims=3, data_range=1.0)  # Heatmaps normalized to [0,1]\n",
    "\n",
    "        self.lambda_focal = lambda_focal\n",
    "        self.lambda_ssim = lambda_ssim\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Compute combined loss over multiple anchors.\"\"\"\n",
    "\n",
    "        focal = self.focal_loss(predictions, targets)\n",
    "        ssim = self.ssim_loss(predictions, targets)\n",
    "\n",
    "        total_loss = focal * self.lambda_focal + ssim * self.lambda_ssim\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, pos_weight=5.0, threshold=0.1):\n",
    "    \"\"\"\n",
    "    pos_weight: weight for voxels where target > threshold.\n",
    "    threshold: value above which the voxel is considered \"important\".\n",
    "    \"\"\"\n",
    "    # Create a weight map: high weight for peaks, 1 otherwise.\n",
    "    weight = torch.where(target > threshold, pos_weight, 1.0)\n",
    "    loss = weight * (pred - target) ** 2\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_loss(pred, target, smooth=1.0):\n",
    "    \"\"\"\n",
    "    A soft dice loss that works on continuous targets.\n",
    "    The 'smooth' parameter helps avoid division by zero.\n",
    "    \"\"\"\n",
    "    # Flatten the tensors to compute a global dice score.\n",
    "    pred_flat = pred.contiguous().view(-1)\n",
    "    target_flat = target.contiguous().view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice_score = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return 1 - dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss = SSIMLoss(spatial_dims=3, data_range=1.0)  # Heatmaps in [0,1]\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def loss_function(pred, target):\n",
    "    return 0.7 * mse_loss(pred, target) + 0.3 * ssim_loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(pred, target, alpha=0.7, pos_weight=5.0, threshold=0.1, smooth=1.0):\n",
    "    \"\"\"\n",
    "    alpha: weight between weighted MSE and Dice loss.\n",
    "           (alpha=1.0 would be pure weighted MSE, while alpha=0.0 would be pure dice loss)\n",
    "    \"\"\"\n",
    "    mse = weighted_mse_loss(pred, target, pos_weight=pos_weight, threshold=threshold)\n",
    "    dice = soft_dice_loss(pred, target, smooth=smooth)\n",
    "    return alpha * mse + (1 - alpha) * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_mse_loss(pred, target, gamma=2.0):\n",
    "    \"\"\"\n",
    "    A focal variant of MSE loss.\n",
    "    The term (|pred - target|)^gamma increases the weight of samples with larger errors.\n",
    "    \"\"\"\n",
    "    mse = (pred - target) ** 2\n",
    "    focal_weight = torch.abs(pred - target) ** gamma\n",
    "    loss = focal_weight * mse\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(pred, target, alpha=0.7, gamma=2.0, smooth=1.0):\n",
    "    focal_mse = focal_mse_loss(pred, target, gamma=gamma)\n",
    "    dice = soft_dice_loss(pred, target, smooth=smooth)\n",
    "    return alpha * focal_mse + (1 - alpha) * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_pixles = heatmaps_array.shape[0] * heatmaps_array.shape[1] * heatmaps_array.shape[2] * heatmaps_array.shape[3] * heatmaps_array.shape[4]\n",
    "total_num_pixles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_pixels = np.sum(heatmaps_array > 0.05)\n",
    "num_pos_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = total_num_pixles / num_pos_pixels\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(pred, target, pos_weight=100000, threshold=0.0):\n",
    "    C = pred.shape[1]\n",
    "    channel_losses = []\n",
    "    channel_weights = [1.0, 2.0, 1.0, 2.0, 1.0]\n",
    "    \n",
    "    for c in range(C):\n",
    "        t = target[:, c, ...]\n",
    "        p = pred[:, c, ...]\n",
    "        #weight = torch.where(t > threshold, torch.tensor(pos_weight, device=t.device), torch.tensor(1.0, device=t.device))\n",
    "        \n",
    "        weight = torch.exp(15 * t)\n",
    "\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.imshow(weight[0][80].cpu().numpy())\n",
    "        plt.show()\n",
    "        '''\n",
    "        \n",
    "        mse = (weight * (p - t) ** 2).mean()\n",
    "        mse = mse * channel_weights[c]        \n",
    "        channel_losses.append(mse)\n",
    "    return sum(channel_losses) / C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedHybridLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.ssim = SSIMLoss(spatial_dims=3, data_range=1.0)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma  # Focal weighting\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 1. Compute MSE with focal weighting\n",
    "        mse = (pred - target)**2\n",
    "        focal_weight = (1 - torch.abs(pred - target))**self.gamma  # Downweight easy regions\n",
    "        weighted_mse = (focal_weight * mse).mean()\n",
    "\n",
    "        # 2. SSIM loss (structural alignment)\n",
    "        ssim_loss = self.ssim(pred, target)\n",
    "\n",
    "        # 3. Combine with emphasis on MSE\n",
    "        return self.alpha * weighted_mse + (1 - self.alpha) * ssim_loss\n",
    "\n",
    "loss_function = WeightedHybridLoss(alpha=0.7, gamma=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from torch.nn import Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetWithSigmoid(UNet):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.out_activation = Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        return self.out_activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "num_res_units = 3\n",
    "prediction_threshold = 0.05\n",
    "#validation_experiment = 'TS_86_3'\n",
    "dropout = 0.3\n",
    "first_channel_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetWithSigmoid(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=num_classes,\n",
    "    channels=(first_channel_size, first_channel_size*2, first_channel_size*2**2, first_channel_size*2**3),\n",
    "    strides=(2, 2, 2),\n",
    "    num_res_units=num_res_units,\n",
    "    dropout=dropout,\n",
    "    kernel_size=5,\n",
    ").to(device)\n",
    "\n",
    "# Add sigmoid activation as a separate layer\n",
    "#model.add_module(\"out_activation\", Sigmoid())\n",
    "\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-5)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "#autocast_dtype = torch.bfloat16\n",
    "\n",
    "best_val_score = 0.0\n",
    "best_argmax_score = 0.0\n",
    "\n",
    "max_epochs = 10000\n",
    "\n",
    "writer = SummaryWriter(\"runs/experiment1\")\n",
    "\n",
    "early_stopping_count = 0\n",
    "early_stopping_rounds = 100\n",
    "\n",
    "best_model = None\n",
    "best_argmax_model = None\n",
    "\n",
    "for epoch in range(max_epochs):        \n",
    "    print(\"-\" * 10)\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        inputs = batch_data[0].to(device, non_blocking=True)\n",
    "        labels = batch_data[1].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #with torch.amp.autocast(device_type='cuda', dtype=autocast_dtype):\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            \n",
    "            # Input 1\n",
    "            plt.figure()\n",
    "            plt.imshow(inputs[0][0][80].cpu().numpy())\n",
    "            plt.show()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 5, figsize=(28, 4))  # Adjust figsize to control spacing\n",
    "\n",
    "            # label 1\n",
    "            axes[0].imshow(labels[0][0][80].cpu().numpy())\n",
    "            axes[0].set_title('Label 1')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # label 2\n",
    "            axes[1].imshow(labels[0][1][80].cpu().numpy())\n",
    "            axes[1].set_title('Label 2')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # label 3\n",
    "            axes[2].imshow(labels[0][2][80].cpu().numpy())\n",
    "            axes[2].set_title('Label 3')\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            # label 4\n",
    "            axes[3].imshow(labels[0][3][80].cpu().numpy())\n",
    "            axes[3].set_title('Label 4')\n",
    "            axes[3].axis('off')\n",
    "\n",
    "            # label 5\n",
    "            axes[4].imshow(labels[0][4][80].cpu().numpy())\n",
    "            axes[4].set_title('Label 5')\n",
    "            axes[4].axis('off')\n",
    "\n",
    "            # Reduce spacing\n",
    "            plt.subplots_adjust(wspace=0.05)  # Reduce horizontal space between images\n",
    "            plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)  # Further minimize padding\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 5, figsize=(28, 4))  # Adjust figsize to control spacing\n",
    "\n",
    "            # prediction 1\n",
    "            axes[0].imshow(outputs[0][0][80].cpu().detach().numpy())\n",
    "            axes[0].set_title('Prediction 1')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # prediction 2\n",
    "            axes[1].imshow(outputs[0][1][80].cpu().detach().numpy())\n",
    "            axes[1].set_title('Prediction 2')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # prediction 3\n",
    "            axes[2].imshow(outputs[0][2][80].cpu().detach().numpy())\n",
    "            axes[2].set_title('Prediction 3')\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            # prediction 4\n",
    "            axes[3].imshow(outputs[0][3][80].cpu().detach().numpy())\n",
    "            axes[3].set_title('Prediction 4')\n",
    "            axes[3].axis('off')\n",
    "\n",
    "            # prediction 5\n",
    "            axes[4].imshow(outputs[0][4][80].cpu().detach().numpy())\n",
    "            axes[4].set_title('Prediction 5')\n",
    "            axes[4].axis('off')\n",
    "            \n",
    "            # Reduce spacing\n",
    "            plt.subplots_adjust(wspace=0.05)  # Reduce horizontal space between images\n",
    "            plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)  # Further minimize padding\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #with torch.amp.autocast(device_type='cuda', dtype=autocast_dtype):\n",
    "        for val_batch_idx, batch_data in enumerate(val_loader):\n",
    "            inputs = batch_data[0].to(device, non_blocking=True)\n",
    "            labels = batch_data[1].to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "        '''\n",
    "        solution_dfs_list = []\n",
    "        submission_dfs_list = []\n",
    "        argmax_submission_dfs_list = []\n",
    "        exp_names_list = []\n",
    "\n",
    "        for first_i in range(len(val_files_copy)):\n",
    "            val_data = val_files_copy[first_i]\n",
    "\n",
    "            exp_name = val_data['label_df']['experiment'][0]\n",
    "            if exp_name in exp_names_list:\n",
    "                continue\n",
    "            else:\n",
    "                exp_names_list.append(exp_name)\n",
    "\n",
    "            tomo = val_data['image']\n",
    "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], cube_size)\n",
    "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0, progress=False, num_workers=8)\n",
    "            \n",
    "            avg_probs_list = []\n",
    "            for second_i in range(len(tomo_ds)):\n",
    "                input_tensor = tomo_ds[second_i]['image'].unsqueeze(0).to(\"cuda\", non_blocking=True)\n",
    "                avg_probs = ensemble_prediction_tta(model, input_tensor)\n",
    "                avg_probs_list.append(avg_probs.cpu().numpy())\n",
    "\n",
    "            reconstructed_tensor = reconstruct_prediction_array(avg_probs_list, coordinates, tomo.shape)\n",
    "            argmax_tensor = torch.argmax(reconstructed_tensor, dim=0)\n",
    "            thresh_probs = reconstructed_tensor > prediction_threshold\n",
    "            _, max_classes = thresh_probs.max(dim=0)\n",
    "            \n",
    "            reconstructed_mask = max_classes.cpu().numpy()\n",
    "            argmax_mask = argmax_tensor.cpu().numpy()\n",
    "\n",
    "            if first_i == 0:\n",
    "                # Create figure\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(16, 4))  # Adjust figsize to control spacing\n",
    "                \n",
    "                # Image 1\n",
    "                axes[0].imshow(tomo[100])\n",
    "                axes[0].set_title('Tomogram')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Image 2\n",
    "                axes[1].imshow(val_data['label'][100])\n",
    "                axes[1].set_title('Painted Segmentation')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Image 3\n",
    "                axes[2].imshow(reconstructed_mask[100])\n",
    "                axes[2].set_title('Threshold Prediction')\n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                # Image 4\n",
    "                axes[3].imshow(argmax_mask[100])\n",
    "                axes[3].set_title('Argmax Prediction')\n",
    "                axes[3].axis('off')\n",
    "                \n",
    "                # Reduce spacing\n",
    "                plt.subplots_adjust(wspace=0.05)  # Reduce horizontal space between images\n",
    "                plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)  # Further minimize padding\n",
    "                \n",
    "                plt.show()\n",
    "            \n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:]\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > 255]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, val_data['label_df']['experiment'][0])\n",
    "\n",
    "            solution_df = val_data['label_df']\n",
    "\n",
    "            solution_dfs_list.append(solution_df)\n",
    "            submission_dfs_list.append(df)\n",
    "\n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(argmax_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:]\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > 255]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, val_data['label_df']['experiment'][0])\n",
    "            argmax_submission_dfs_list.append(df)\n",
    "\n",
    "        solution_concat_df = pd.concat(solution_dfs_list, ignore_index=True).reset_index(drop=True)#.reset_index().rename(columns={'index':'id'})[['id', 'experiment', 'particle_type', 'x', 'y', 'z']]\n",
    "        submission_concat_df = pd.concat(submission_dfs_list, ignore_index=True).reset_index(drop=True)#.reset_index().rename(columns={'index':'id'})\n",
    "        argmax_submission_concat_df = pd.concat(argmax_submission_dfs_list, ignore_index=True).reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        particle_names = ['apo-ferritin', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
    "        particle_radius = {\n",
    "            'apo-ferritin': 65,\n",
    "            'beta-galactosidase': 95,\n",
    "            'ribosome': 150,\n",
    "            'thyroglobulin': 135,\n",
    "            'virus-like-particle': 145,\n",
    "        }\n",
    "        \n",
    "        val_fbeta_score = score(solution_concat_df, submission_concat_df, 'id', 0.1*0.5, 4)\n",
    "        \n",
    "        argmax_val_fbeta_score = score(solution_concat_df, argmax_submission_concat_df, 'id', 0.1*0.5, 4)\n",
    "\n",
    "        if val_fbeta_score > best_val_score:\n",
    "            best_val_score = val_fbeta_score\n",
    "            early_stopping_count = 0\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(best_model.state_dict(), os.path.join(output_path, f\"best_threshold_model_{notebook_number}.pth\"))\n",
    "\n",
    "        if argmax_val_fbeta_score > best_argmax_score:\n",
    "            best_argmax_score = argmax_val_fbeta_score\n",
    "            early_stopping_count = 0\n",
    "            best_argmax_model = copy.deepcopy(model)\n",
    "            torch.save(best_argmax_model.state_dict(), os.path.join(output_path, f\"best_argmax_model_{notebook_number}.pth\"))\n",
    "        '''\n",
    "\n",
    "    early_stopping_count += 1\n",
    "\n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Validation\": val_loss}, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    #print(f\"Val Threshold F beta score: {val_fbeta_score:.4f}, Val Argmax F beta score: {argmax_val_fbeta_score:.4f}, Best Threshold Val F beta score: {best_val_score:.4f}, Best Argmax Val F beta score: {best_argmax_score:.4f}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    #break\n",
    "    \n",
    "    if early_stopping_count >= early_stopping_rounds:\n",
    "        print('Early Stopping Triggered.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://www.kaggle.com/code/hideyukizushi/czii-yolo11-unet3d-monai-lb-707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
