{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":10392564,"sourceType":"datasetVersion","datasetId":6438716}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport os\nimport gc\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor, log_evaluation\nfrom IPython.display import clear_output\nfrom sklearn.metrics import r2_score\nimport time\n\ngc.enable()\n\n'''\npd.options.display.max_columns = None\n#pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\n\n#pl.Config.set_tbl_rows(-1)\npl.Config.set_tbl_cols(-1)\npl.Config.set_fmt_str_lengths(10000)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:34:56.896929Z","iopub.execute_input":"2025-01-12T07:34:56.897234Z","iopub.status.idle":"2025-01-12T07:35:00.982814Z","shell.execute_reply.started":"2025-01-12T07:34:56.897207Z","shell.execute_reply":"2025-01-12T07:35:00.981818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom pathlib import Path\nDATA_DIR = Path('/kaggle/input/jane-street-real-time-market-data-forecasting')\n\ndate_offset = 1500\n\nis_score_dates = 5\n\npl_all = pl.scan_parquet(DATA_DIR/\"train.parquet\").filter(pl.col(\"date_id\") >= date_offset-1).collect()\n\n# make syn_test \nsyn_test = pl_all.with_columns(\n    # pl.lit(True).alias(\"is_scored\"),\n    pl.col('date_id') - date_offset\n    ).with_row_index(name=\"row_id\", offset=0)\n\nsyn_test = syn_test.with_columns(\n    pl.when(pl.col('date_id')<is_score_dates-1).then(pl.lit(False)).otherwise(pl.lit(True)).alias(\"is_scored\")\n)\n\nsyn_test = syn_test.select(\n    ['row_id', 'date_id', 'time_id', 'symbol_id', 'weight', 'is_scored'] + [f'feature_{x:02}' for x in range(79)]\n)\n\nsyn_test_partition = syn_test.partition_by('date_id', maintain_order=True, as_dict=True)\n\noutput_dir = \"synthetic_test.parquet\"\nos.makedirs(output_dir, exist_ok=True)\n\nrow_id_offset = syn_test.filter(pl.col('date_id')<0).select('row_id').max().item()\nprint(\"row_id_offset:\", row_id_offset)\n\nfor key, _df in syn_test_partition.items():\n    if key[0] >= 0:\n        os.makedirs(f\"{output_dir}/date_id={key[0]}\", exist_ok=True)\n        _df = _df.with_columns(pl.col('row_id')-row_id_offset)\n        _df.write_parquet(f\"{output_dir}/date_id={key[0]}/part-0.parquet\")\n\n# make syn_lag\n\nsyn_lag = pl_all.select(\n    ['date_id', 'time_id', 'symbol_id'] + [f'responder_{x}' for x in range(9)]\n).with_columns(pl.col('date_id')-date_offset)\n\nsyn_lag = syn_lag.rename({f'responder_{x}': f'responder_{x}_lag_1' for x in range(9)})\n\nsyn_lag_partition = syn_lag.partition_by('date_id', maintain_order=True, as_dict=True)\n\noutput_dir = \"synthetic_lags.parquet\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor key, _df in syn_lag_partition.items():\n    os.makedirs(f\"{output_dir}/date_id={key[0]+1}\", exist_ok=True)\n    _df = _df.with_columns(pl.col('date_id')+1)\n    _df.write_parquet(f\"{output_dir}/date_id={key[0]+1}/part-0.parquet\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:00.983938Z","iopub.execute_input":"2025-01-12T07:35:00.984392Z","iopub.status.idle":"2025-01-12T07:35:00.989856Z","shell.execute_reply.started":"2025-01-12T07:35:00.984371Z","shell.execute_reply":"2025-01-12T07:35:00.988930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/input/jane-street-real-time-market-data-forecasting/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:00.991956Z","iopub.execute_input":"2025-01-12T07:35:00.992247Z","iopub.status.idle":"2025-01-12T07:35:01.011439Z","shell.execute_reply.started":"2025-01-12T07:35:00.992220Z","shell.execute_reply":"2025-01-12T07:35:01.010686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"schema_df = pl.read_csv('/kaggle/input/schema/schema.csv')\nschema_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.012548Z","iopub.execute_input":"2025-01-12T07:35:01.012816Z","iopub.status.idle":"2025-01-12T07:35:01.079332Z","shell.execute_reply.started":"2025-01-12T07:35:01.012788Z","shell.execute_reply":"2025-01-12T07:35:01.078357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = {\n    'val_window_size': 8,\n    'training_window_size': 404,\n    'learning_rate': 0.04588738403235412,\n    'max_depth': 12,\n    'min_data_in_leaf': 60,\n    'num_leaves': 4763,\n    'min_gain_to_split': 0.25,\n    'lambda_l1': 4.0,\n    'lambda_l2': 1786.5166849320328,\n    'feature_fraction': 0.9547872173111335\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.080177Z","iopub.execute_input":"2025-01-12T07:35:01.080420Z","iopub.status.idle":"2025-01-12T07:35:01.084445Z","shell.execute_reply.started":"2025-01-12T07:35:01.080381Z","shell.execute_reply":"2025-01-12T07:35:01.083390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_window_size = best_params['val_window_size']\ntraining_window_size = best_params['training_window_size']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.085396Z","iopub.execute_input":"2025-01-12T07:35:01.085767Z","iopub.status.idle":"2025-01-12T07:35:01.100272Z","shell.execute_reply.started":"2025-01-12T07:35:01.085737Z","shell.execute_reply":"2025-01-12T07:35:01.099359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"whole_size = val_window_size + training_window_size\nwhole_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.101147Z","iopub.execute_input":"2025-01-12T07:35:01.101566Z","iopub.status.idle":"2025-01-12T07:35:01.116715Z","shell.execute_reply.started":"2025-01-12T07:35:01.101536Z","shell.execute_reply":"2025-01-12T07:35:01.115807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#temp_df = pl.read_parquet(path + 'train.parquet/', columns=schema_df.columns).select(pl.all().shrink_dtype()).filter((pl.col('date_id') < 1499)&(pl.col('date_id') >= 1499 - whole_size))\n#temp_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.119544Z","iopub.execute_input":"2025-01-12T07:35:01.119845Z","iopub.status.idle":"2025-01-12T07:35:01.129373Z","shell.execute_reply.started":"2025-01-12T07:35:01.119816Z","shell.execute_reply":"2025-01-12T07:35:01.128529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#date_id_max = temp_df['date_id'].max()\n#date_id_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.130790Z","iopub.execute_input":"2025-01-12T07:35:01.131068Z","iopub.status.idle":"2025-01-12T07:35:01.142681Z","shell.execute_reply.started":"2025-01-12T07:35:01.131040Z","shell.execute_reply":"2025-01-12T07:35:01.141899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"date_id_max = pl.scan_parquet(path + 'train.parquet/').select('date_id').max().collect()['date_id'][0]\n#date_id_max","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.143369Z","iopub.execute_input":"2025-01-12T07:35:01.143662Z","iopub.status.idle":"2025-01-12T07:35:01.549651Z","shell.execute_reply.started":"2025-01-12T07:35:01.143635Z","shell.execute_reply":"2025-01-12T07:35:01.548934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"starting_date = date_id_max - val_window_size - training_window_size\nstarting_date","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.550353Z","iopub.execute_input":"2025-01-12T07:35:01.550604Z","iopub.status.idle":"2025-01-12T07:35:01.555134Z","shell.execute_reply.started":"2025-01-12T07:35:01.550584Z","shell.execute_reply":"2025-01-12T07:35:01.554533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntrain_df = temp_df.filter(pl.col('date_id') > starting_date)\ndel temp_df\ngc.collect()\n#train_df\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.555916Z","iopub.execute_input":"2025-01-12T07:35:01.556148Z","iopub.status.idle":"2025-01-12T07:35:01.570667Z","shell.execute_reply.started":"2025-01-12T07:35:01.556129Z","shell.execute_reply":"2025-01-12T07:35:01.569990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_dtype(dtype_str):\n    if dtype_str == 'Int16':\n        return pl.Int16\n    elif dtype_str == 'Int8':\n        return pl.Int8\n    elif dtype_str == 'Float32':\n        return pl.Float32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.571509Z","iopub.execute_input":"2025-01-12T07:35:01.571784Z","iopub.status.idle":"2025-01-12T07:35:01.585545Z","shell.execute_reply.started":"2025-01-12T07:35:01.571752Z","shell.execute_reply":"2025-01-12T07:35:01.584728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_df = pl.read_parquet(path + 'train.parquet/', columns=schema_df.columns).select(pl.all().shrink_dtype()).filter(pl.col('date_id') > starting_date)\ntrain_df = train_df.with_columns([\n    pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in train_df.columns\n])\n#train_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:01.586370Z","iopub.execute_input":"2025-01-12T07:35:01.586666Z","iopub.status.idle":"2025-01-12T07:35:40.530628Z","shell.execute_reply.started":"2025-01-12T07:35:01.586631Z","shell.execute_reply":"2025-01-12T07:35:40.529715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['date_id'].n_unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.531535Z","iopub.execute_input":"2025-01-12T07:35:40.531829Z","iopub.status.idle":"2025-01-12T07:35:40.584242Z","shell.execute_reply.started":"2025-01-12T07:35:40.531801Z","shell.execute_reply":"2025-01-12T07:35:40.583266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.estimated_size() / 1e9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.585228Z","iopub.execute_input":"2025-01-12T07:35:40.585613Z","iopub.status.idle":"2025-01-12T07:35:40.601475Z","shell.execute_reply.started":"2025-01-12T07:35:40.585573Z","shell.execute_reply":"2025-01-12T07:35:40.600811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kaggle_evaluation.jane_street_inference_server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.602316Z","iopub.execute_input":"2025-01-12T07:35:40.602591Z","iopub.status.idle":"2025-01-12T07:35:40.861165Z","shell.execute_reply.started":"2025-01-12T07:35:40.602572Z","shell.execute_reply":"2025-01-12T07:35:40.859946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TimerCallback:\n    def __init__(self, max_time_seconds, loop_start_time):\n        self.max_time_seconds = max_time_seconds\n        #self.start_time = None\n        self.loop_start_time = loop_start_time\n\n    def __call__(self, env):\n        #if self.start_time is None:\n        #    self.start_time = time.time()\n\n        elapsed_time = time.time() - self.loop_start_time\n        if elapsed_time > self.max_time_seconds:\n            print(f\"Stopping training after {elapsed_time:.2f} seconds.\")\n            best_iteration = env.model.best_iteration\n            best_score = env.model.best_score\n            raise lgb.callback.EarlyStopException(best_iteration, best_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.862017Z","iopub.execute_input":"2025-01-12T07:35:40.862828Z","iopub.status.idle":"2025-01-12T07:35:40.871933Z","shell.execute_reply.started":"2025-01-12T07:35:40.862787Z","shell.execute_reply":"2025-01-12T07:35:40.871201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lags_df = None\nstreaming_data_y_list = []\nstreaming_data_X_list = []\n#streaming_data_lags_list = []\ny_concat = None\nX_concat = None\nstreaming_data_X_concat_list = []\ntemp_i = 0\nmodel = None\n\n# Replace this function with your inference code.\n# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n    # Use them as extra features, if you like.\n    \n    global lags_df, streaming_data_y_list, streaming_data_X_list, temp_i, streaming_data_lags_list, model, y_concat, X_concat, streaming_data_X_concat_list, train_df\n\n    test_df = test.drop(['row_id', 'is_scored'])\n    test_df = test_df.with_columns([\n        pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in test_df.columns\n    ])    \n\n    streaming_data_X_list.append(test_df)\n    \n    if lags is not None:\n        timer_start_time = time.time()\n        date_id = lags['date_id'][0]\n        temp_date_id = date_id_max + date_id\n        lags_df = lags\n        \n        y_df = lags_df.select(['date_id', 'time_id', 'symbol_id', 'responder_6_lag_1']).with_columns(pl.col('date_id') - 1).rename({'responder_6_lag_1':'responder_6'})\n        y_df = y_df.with_columns([\n            pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in y_df.columns\n        ])        \n        streaming_data_y_list.append(y_df)\n\n        y_concat = pl.concat(streaming_data_y_list)\n        X_concat = pl.concat(streaming_data_X_list)\n        X_concat = X_concat.with_columns([\n            pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in X_concat.columns\n        ])        \n        streaming_data_X_concat_list.append(X_concat)\n        streaming_data_X_list = []\n\n        if len(streaming_data_X_concat_list) > 2:\n            del streaming_data_X_concat_list[0], streaming_data_y_list[0]\n\n        val_date_id_cut_lower = date_id - val_window_size        \n        training_date_id_cut_lower = val_date_id_cut_lower - training_window_size\n\n        streaming_data_X_concat_concat = pl.concat(streaming_data_X_concat_list)\n        streaming_data_X_concat_concat = streaming_data_X_concat_concat.with_columns([\n            pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in streaming_data_X_concat_concat.columns\n        ])\n        \n        streaming_data_df = streaming_data_X_concat_concat.join(y_concat, on=['date_id', 'time_id', 'symbol_id'], how='left').drop_nulls(subset=['responder_6'])\n        streaming_data_df = streaming_data_df.with_columns([\n            pl.col(col).cast(find_dtype(schema_df[col][0])) for col in schema_df.columns if col in streaming_data_df.columns\n        ])\n        streaming_data_df = streaming_data_df.filter(pl.col('date_id') == date_id - 1).with_columns(pl.col('date_id') + date_id_max + 1)\n\n        starting_date = date_id_max - val_window_size - training_window_size + date_id\n        train_df = train_df.filter(pl.col('date_id') > starting_date)\n        train_df = pl.concat([train_df, streaming_data_df]).select(schema_df.columns)\n        #display(train_df)\n\n        val_data_df = train_df.filter(pl.col('date_id') > train_df['date_id'].max() - val_window_size)\n        #print('this is val_data_df')\n        #display(val_data_df)\n        #print('val_data_df date_id n_unique:', val_data_df['date_id'].n_unique())\n\n        train_data_df = train_df.filter(pl.col('date_id') <= train_df['date_id'].max() - val_window_size).sample(n=1100000)\n        #print('this is train_data_df')\n        #display(train_data_df)\n        #print('train_data_df date_id n_unique:', train_data_df['date_id'].n_unique())\n        \n        \n        \n        base_params = {\n            'verbosity': -1,\n            'device': 'gpu',\n            'early_stopping_round': 20,\n        }\n    \n        tuned_params = {\n            'learning_rate': best_params['learning_rate'],\n            'max_depth': best_params['max_depth'],\n            'min_data_in_leaf': best_params['min_data_in_leaf'],\n            'num_leaves': best_params['num_leaves'],\n            'min_gain_to_split': best_params['min_gain_to_split'],\n            'lambda_l1': best_params['lambda_l1'],\n            'lambda_l2': best_params['lambda_l2'],\n            'feature_fraction': best_params['feature_fraction'],\n        }\n    \n        model = LGBMRegressor(\n            **base_params,\n            **tuned_params,\n            n_estimators=100000\n        )\n        \n\n        X_train = train_data_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n        X_val = val_data_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n        #X_test = test_data_df.drop(['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']).select(pl.all().shrink_dtype()).to_pandas()\n    \n        y_train = train_data_df.select('responder_6').to_series().to_pandas()\n        y_val = val_data_df.select('responder_6').to_series().to_pandas()\n        #y_test = test_data_df.select('responder_6').to_series().to_pandas()\n    \n        weights_train = train_data_df.select('weight').to_series().to_pandas()\n        weights_val = val_data_df.select('weight').to_series().to_pandas()\n        #weights_test = test_data_df.select('weight').to_series().to_pandas()\n\n        if temp_i == 0:\n            timer_callback = TimerCallback(max_time_seconds=110, loop_start_time=timer_start_time)\n        else:\n            timer_callback = TimerCallback(max_time_seconds=58, loop_start_time=timer_start_time)\n\n        training_timer_start_time = time.time()\n        try:\n            #model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val])#, callbacks=[log_evaluation(period=10)])\n            model.fit(X_train, y_train, sample_weight=weights_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_sample_weight=[weights_train, weights_val], callbacks=[timer_callback])\n        except lgb.callback.EarlyStopException as e:\n            print(f\"Training stopped. Best iteration: {e.best_iteration}, Best score: {e.best_score}\")\n        training_timer_end_time = time.time()\n        training_time = training_timer_end_time - training_timer_start_time\n        print(f\"Training time: {training_time:.2f} seconds\")\n        \n        #val_preds = model.predict(X_val)\n        #val_score = r2_score(y_val, val_preds, sample_weight=weights_val)\n        #print('Val Weighted R2 score is:', val_score)\n        \n\n        #del X_train, y_train, X_val, y_val, weights_train, weights_val, window_concat, window_df, concat_join_df, y_concat, X_concat\n        gc.collect()\n                \n        temp_i += 1\n        timer_end_time = time.time()\n\n        execution_time = timer_end_time - timer_start_time\n        print(f\"Execution time: {execution_time:.2f} seconds\")\n        #print('streaming_data_X_concat_list len:', len(streaming_data_X_concat_list))\n        #print('streaming_data_y_list len:', len(streaming_data_y_list))\n\n\n    #\n    test_df = test_df.drop(['date_id', 'time_id', 'symbol_id', 'weight']).select(pl.all().shrink_dtype()).to_numpy()\n    \n    #\n    preds = model.predict(test_df)\n\n    predictions = test.select(\n        'row_id',\n        pl.lit(preds).alias('responder_6'),\n    )\n\n    if isinstance(predictions, pl.DataFrame):\n        assert predictions.columns == ['row_id', 'responder_6']\n    elif isinstance(predictions, pd.DataFrame):\n        assert (predictions.columns == ['row_id', 'responder_6']).all()\n    else:\n        raise TypeError('The predict function must return a DataFrame')\n    # Confirm has as many rows as the test data.\n    assert len(predictions) == len(test)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.872594Z","iopub.execute_input":"2025-01-12T07:35:40.872868Z","iopub.status.idle":"2025-01-12T07:35:40.927744Z","shell.execute_reply.started":"2025-01-12T07:35:40.872842Z","shell.execute_reply":"2025-01-12T07:35:40.926931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%time\ninference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            #'/kaggle/working/synthetic_test.parquet',\n            #'/kaggle/working/synthetic_lags.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n        \n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T07:35:40.928564Z","iopub.execute_input":"2025-01-12T07:35:40.928787Z","iopub.status.idle":"2025-01-12T07:36:21.615550Z","shell.execute_reply.started":"2025-01-12T07:35:40.928768Z","shell.execute_reply":"2025-01-12T07:36:21.614650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}