{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Update\n* Version 2: set thresholds for previous_records_df based on temp_test_X accuracy_group distribution","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.\nfrom time import time\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom scipy import stats\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nimport gc\nimport json\npd.set_option('display.max_columns', 1000)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nrandom.seed(42)\nnp.random.seed(42)\nfrom scipy.stats import rankdata","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#source: https://www.kaggle.com/artgor/quick-and-dirty-regression\nfrom functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -cohen_kappa_score(y, X_p, weights='quadratic')\n\n    def fit(self, X, y, method='Powell', initial_coefficients=None):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [np.percentile(X, 25), np.percentile(X, 50), np.percentile(X, 75)]\n        if initial_coefficients:\n            initial_coef = initial_coefficients\n        print('initial coefficients:', initial_coef)\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method=method)\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_data():\n    base_path = '/kaggle/input/data-science-bowl-2019/'\n    date_format =  '%Y-%m-%dT%H:%M:%S.%fZ'\n    print('Reading train.csv file....')\n    train = pd.read_csv(base_path+'train.csv', parse_dates=[2], infer_datetime_format=date_format)\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv(base_path+'test.csv', parse_dates=[2], infer_datetime_format=date_format)\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv(base_path+'train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv(base_path+'specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv(base_path+'sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\nconcat_df = pd.concat([train, test])\ngame_titles = concat_df[concat_df['type'] == 'Game']['title'].unique()\nactivity_titles = concat_df[concat_df['type'] == 'Activity']['title'].unique()\nassessment_titles = concat_df[concat_df['type'] == 'Assessment']['title'].unique()\nclip_titles = concat_df[concat_df['type'] == 'Clip']['title'].unique()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    inv_title_map = {v:k for k,v in activities_map.items()}\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    '''train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])'''\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, inv_title_map, activities_map","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, inv_title_map, activities_map = encode_title(train, test, train_labels)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def categorise_accuracy_group(x):\n    if x >= 1:\n        return 3\n    elif (x < 1) & (x >= 0.5):\n        return 2\n    elif (x < 0.5) & (x > 0):\n        return 1\n    elif x <= 0:\n        return 0\n    else:\n        raise ValueError('something went wrong')","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):#, title_value_counts_dict=None):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    user_sample = user_sample.sort_values(['timestamp', 'title', 'game_session', 'event_count']).reset_index(drop=True)\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    game_data = dict(zip(game_titles, np.zeros(len(game_titles))))\n    activity_data = dict(zip(activity_titles, np.zeros(len(activity_titles))))\n    assessment_data = dict(zip(assessment_titles, np.zeros(len(assessment_titles))))\n    clip_data = dict(zip(clip_titles, np.zeros(len(clip_titles))))\n    \n    game_correct_attempts = {key+'_correct_attempts':0 for key in game_data.keys()}\n    game_incorrect_attempts = {key+'_incorrect_attempts':0 for key in game_data.keys()}\n    \n    game_correct_attempts_list_dict = {key+'_correct_attempts_list':[] for key in game_data.keys()}\n    game_incorrect_attempts_list_dict = {key+'_incorrect_attempts_list':[] for key in game_data.keys()}\n    \n    assessment_correct_attempts = {key+'_correct_attempts':0 for key in assessment_data.keys()}\n    assessment_incorrect_attempts = {key+'_incorrect_attempts':0 for key in assessment_data.keys()}\n    assessment_total_attempts_dict = {key+'_total_attempts':0 for key in assessment_data.keys()}\n    \n    assessment_correct_attempts_list_dict = {key+'_correct_attempts_list':[] for key in assessment_data.keys()}\n    assessment_incorrect_attempts_list_dict = {key+'_incorrect_attempts_list':[] for key in assessment_data.keys()}\n    assessment_total_attempts_list_dict = {key+'_total_attempts_list':[] for key in assessment_data.keys()}\n    assessment_accuracy_list_dict = {key+'_accuracy_list':[] for key in assessment_data.keys()}\n    \n    activity_playtime = {key+'_playtime':0 for key in activity_data.keys()}\n    \n    game_accuracies_dict = {key+'_accuracy':[] for key in game_data.keys()}\n    assessment_durations_dict = {key+'_duration':[] for key in assessment_data.keys()}\n    \n    game_durations_dict = {key+'_duration':[] for key in game_data.keys()}\n    game_playtime_dict = {key+'_playtime':0 for key in game_data.keys()}\n    assessment_playtime_dict = {key+'_playtime':0 for key in assessment_data.keys()}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_pseudo_correct_attempts = 0\n    accumulated_pseudo_incorrect_attempts = 0\n    accumulated_game_correct_attempts = 0\n    accumulated_game_incorrect_attempts = 0\n    accumulated_assessment_correct_attempts = 0\n    accumulated_assessment_incorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    game_durations = []\n    activity_durations = []\n    assessment_durations = []\n    game_accuracies_list = []\n    assessment_accuracies_list = []\n    activity_4070_list = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        session = session.sort_values(['event_count']).reset_index(drop=True)\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        session_duration = (session.iloc[-1, 2] - session.iloc[0, 2] ).seconds                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            pseudo_true_attempts = session['event_data'].str.contains('true').sum()\n            pseudo_false_attempts = session['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            '''features.update(game_correct_attempts.copy())\n            features.update(game_incorrect_attempts.copy())\n            features.update(activity_playtime.copy())'''\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n            # time features\n            features['hour'] = session['timestamp'].iloc[0].hour\n            features['day'] = session['timestamp'].iloc[0].day\n            features['dayofweek'] = session['timestamp'].iloc[0].dayofweek\n            features['week'] = session['timestamp'].iloc[0].week\n            features['month'] = session['timestamp'].iloc[0].month\n            features['year'] = session['timestamp'].iloc[0].year\n            \n            #\n            features['accumulated_pseudo_correct_attempts'] = accumulated_pseudo_correct_attempts\n            features['accumulated_pseudo_incorrect_attempts'] = accumulated_pseudo_incorrect_attempts\n            accumulated_pseudo_correct_attempts += pseudo_true_attempts \n            accumulated_pseudo_incorrect_attempts += pseudo_false_attempts\n            \n            #\n            features['accumulated_game_correct_attempts'] = accumulated_game_correct_attempts\n            features['accumulated_game_incorrect_attempts'] = accumulated_game_incorrect_attempts\n            features['accumulated_assessment_correct_attempts'] = accumulated_assessment_correct_attempts\n            features['accumulated_assessment_incorrect_attempts'] = accumulated_assessment_incorrect_attempts\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            pseudo_accuracy = pseudo_true_attempts/(pseudo_true_attempts+pseudo_false_attempts) if (pseudo_true_attempts+pseudo_false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            features['accuracy'] = accuracy\n            features['pseudo_accuracy'] = pseudo_accuracy\n            features['pseudo_accuracy_group'] = categorise_accuracy_group(pseudo_accuracy)\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # attempted before\n            if assessment_total_attempts_dict[inv_title_map[session_title]+'_total_attempts'] != 0:\n                features['attempted_before'] = 1\n            else:\n                features['attempted_before'] = 0\n            \n            # solved before\n            if assessment_correct_attempts[inv_title_map[session_title]+'_correct_attempts'] != 0:\n                features['solved_before'] = 1\n            else:\n                features['solved_before'] = 0\n            \n            # if solved before, then previous accuracy group\n            \n            # first accuracy group\n            if assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'] != []:\n                features['first_accuracy'] = assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'][0]\n            else:\n                features['first_accuracy'] = 0\n            \n            # last accuracy group\n            if assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'] != []:\n                features['last_accuracy'] = assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'][-1]\n            else:\n                features['last_accuracy'] = 0\n            \n            # mean accuracy group\n            if assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'] != []:\n                features['mean_previous_accuracy'] = np.mean(assessment_accuracy_list_dict[inv_title_map[session_title]\\\n                                                                                           +'_accuracy_list'])\n            else:\n                features['mean_previous_accuracy'] = 0\n            \n            # std accuracy\n            if assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'] != []:\n                features['std_previous_accuracy'] = np.std(assessment_accuracy_list_dict[inv_title_map[session_title]\\\n                                                                                           +'_accuracy_list'])\n            else:\n                features['std_previous_accuracy'] = 0\n                \n            # number of previous attempts (correct, incorrect, total)\n            features['previous_correct_attempts'] = assessment_correct_attempts[inv_title_map[session_title]\\\n                                                                                +'_correct_attempts']\n            features['previous_incorrect_attempts'] = assessment_incorrect_attempts[inv_title_map[session_title]\\\n                                                                                    +'_incorrect_attempts']\n            features['previous_total_attempts'] = assessment_total_attempts_dict[inv_title_map[session_title]\\\n                                                                                 +'_total_attempts']\n            # previous assessment duration\n            if assessment_durations_dict[inv_title_map[session_title]+'_duration'] != []:\n                features['first_assessment_duration'] = assessment_durations_dict[inv_title_map[session_title]+'_duration'][0]\n                features['last_assessment_duration'] = assessment_durations_dict[inv_title_map[session_title]+'_duration'][-1]\n                features['mean_assessment_duration'] = np.mean(assessment_durations_dict[inv_title_map[session_title]+'_duration'])\n            else:\n                features['first_assessment_duration'] = 0\n                features['last_assessment_duration'] = 0\n                features['mean_assessment_duration'] = 0\n            \n            # previous playtime\n            features['previous_playtime'] = assessment_playtime_dict[inv_title_map[session_title]+'_playtime']\n            \n            # previous attempts and playtime combined\n            features['previous_correct_per_second'] = features['previous_correct_attempts'] / features['previous_playtime'] if features['previous_playtime'] != 0 else 0\n            features['previous_incorrect_per_second'] = features['previous_incorrect_attempts'] / features['previous_playtime'] if features['previous_playtime'] != 0 else 0\n            features['previous_total_per_second'] = features['previous_total_attempts'] / features['previous_playtime'] if features['previous_playtime'] != 0 else 0\n            \n            # game last played\n            \n            # activity last played\n            \n            # class weights for each title\n            '''if title_value_counts_dict:\n                feature_value_counts = title_value_counts_dict[inv_title_map[session_title]]\n                feature_class_weights = [feature_value_counts.min() / feature_value_counts[i] for i in range(len(feature_value_counts))]\n                for i in range(len(feature_value_counts)):\n                    features['feature_value_counts_{}'.format(i)] = feature_value_counts[i]\n                    features['feature_class_weights_{}'.format(i)] = feature_class_weights[i]\n                    #features['accuracy_group_order_{}'.format(i)] = feature_value_counts.index[i]\n                    features['frequency_order_{}'.format(i)] = feature_value_counts.iloc[i]'''\n            \n            # game accuracies feature engineering\n            features['mean_game_accuracy2'] = 0\n            if game_accuracies_list != []:\n                features['mean_game_accuracy2'] = np.mean(game_accuracies_list)\n                \n            features['std_game_accuracy2'] = 0\n            if game_accuracies_list != []:\n                features['std_game_accuracy2'] = np.std(game_accuracies_list)\n                \n            # assessment accuracies feature engineering\n            '''features['mean_assessment_accuracy2'] = 0\n            features['std_assessment_accuracy2'] = 0\n            if assessment_accuracies_list != []:\n                features['mean_assessment_accuracy2'] = np.mean(assessment_accuracies_list)\n                features['std_assessment_accuracy2'] = np.std(assessment_accuracies_list)'''\n                \n            # activity playtime feature engineering\n            features['total_activity_playtime'] = 0\n            features['mean_activity_playtime'] = 0\n            features['std_activity_playtime'] = 0\n            if activity_durations != []:\n                features['total_activity_playtime'] = np.sum(activity_durations)\n                features['mean_activity_playtime'] = np.mean(activity_durations)\n                features['std_activity_playtime'] = np.std(activity_durations)\n                \n            # activity 4070 feature engineering\n            features['total_activity_4070'] = 0\n            features['mean_activity_4070'] = 0\n            features['std_activity_4070'] = 0\n            if activity_4070_list != []:\n                features['total_activity_4070'] = np.sum(activity_4070_list)\n                features['mean_activity_4070'] = np.mean(activity_4070_list)\n                features['std_activity_4070'] = np.std(activity_4070_list)\n            \n            # activity 4070 and time\n            features['activity_4070_per_second'] = features['total_activity_4070'] / features['total_activity_playtime'] if features['total_activity_playtime'] != 0 else 0\n            features['mean_activity_4070_per_second'] = features['mean_activity_4070'] / features['total_activity_playtime'] if features['total_activity_playtime'] != 0 else 0\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n        \n        # game attempt counts\n        if session_type == 'Game':\n            game_true_attempts = session['event_data'].str.contains('\"correct\":true').sum()\n            game_false_attempts = session['event_data'].str.contains('\"correct\":false').sum()\n            current_game_total_attempts = game_true_attempts + game_false_attempts\n            current_game_accuracy = game_true_attempts / current_game_total_attempts if current_game_total_attempts != 0 else 'pass'\n            if current_game_accuracy != 'pass':\n                game_accuracies_dict[inv_title_map[session_title]+'_accuracy'].append(current_game_accuracy)\n                game_accuracies_list.append(current_game_accuracy)\n            \n            accumulated_game_correct_attempts += game_true_attempts\n            accumulated_game_incorrect_attempts += game_false_attempts\n            \n            game_correct_attempts[inv_title_map[session_title]+'_correct_attempts'] += game_true_attempts\n            game_incorrect_attempts[inv_title_map[session_title]+'_incorrect_attempts'] += game_false_attempts\n            \n            game_correct_attempts_list_dict[inv_title_map[session_title]+'_correct_attempts_list'].append(game_true_attempts)\n            game_incorrect_attempts_list_dict[inv_title_map[session_title]+'_incorrect_attempts_list'].append(game_false_attempts)\n            if current_game_total_attempts > 0:\n                game_durations_dict[inv_title_map[session_title]+'_duration'].append(session_duration)\n            \n            game_playtime_dict[inv_title_map[session_title]+'_playtime'] += session_duration\n            \n        elif session_type == 'Activity':\n            playtime = (session.iloc[-1, 2] - session.iloc[0, 2] ).seconds\n            activity_durations.append(playtime)\n            \n            activity_playtime[inv_title_map[session_title]+'_playtime'] += playtime\n            activity_4070 = (session['event_code'] == 4070).sum()\n            activity_4070_list.append(activity_4070)\n        \n        elif session_type == 'Assessment':\n            assessment_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n            if session_title == 'Bird Measurer (Assessment)':\n                attempts_df = session[session['event_code'] == 4110]\n            else:\n                attempts_df = session[session['event_code'] == 4100]\n            assessment_true_attempts = attempts_df['event_data'].str.contains('\"correct\":true').sum()\n            assessment_false_attempts = attempts_df['event_data'].str.contains('\"correct\":false').sum()\n            assessment_total_attempts = assessment_true_attempts + assessment_false_attempts\n            feature_assessment_accuracy = assessment_true_attempts / assessment_total_attempts if assessment_total_attempts != 0 else 'pass'\n            accumulated_assessment_correct_attempts += assessment_true_attempts\n            accumulated_assessment_incorrect_attempts += assessment_false_attempts\n            if assessment_total_attempts > 0:\n                assessment_durations_dict[inv_title_map[session_title]+'_duration'].append(session_duration)\n                assessment_playtime_dict[inv_title_map[session_title]+'_playtime'] += session_duration\n            \n            assessment_correct_attempts[inv_title_map[session_title]+'_correct_attempts'] += assessment_true_attempts\n            assessment_incorrect_attempts[inv_title_map[session_title]+'_incorrect_attempts'] += assessment_false_attempts\n            assessment_total_attempts_dict[inv_title_map[session_title]+'_total_attempts'] += assessment_total_attempts\n            \n            assessment_correct_attempts_list_dict[inv_title_map[session_title]+'_correct_attempts_list'].append(assessment_true_attempts)\n            assessment_incorrect_attempts_list_dict[inv_title_map[session_title]+'_incorrect_attempts_list'].append(assessment_false_attempts)\n            assessment_total_attempts_list_dict[inv_title_map[session_title]+'_total_attempts_list']\\\n            .append(assessment_total_attempts)\n            if feature_assessment_accuracy == 'pass':\n                pass\n            else:\n                assessment_accuracy_list_dict[inv_title_map[session_title]+'_accuracy_list'].append(feature_assessment_accuracy)\n                assessment_accuracies_list.append(feature_assessment_accuracy)\n            \n            \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if all_assessments != []:\n        for features_dict in all_assessments:\n            features_dict['is_last_session'] = 0\n            features_dict['is_first_session'] = 0\n        all_assessments[-1]['is_last_session'] = 1\n        all_assessments[0]['is_first_session'] = 1\n        \n    '''game_accuracies_list = []\n    for k, v in game_accuracies_dict.items():\n        if v != []:\n            game_accuracies_list.append(v[0])'''\n            \n    assessment_durations_list = []\n    for k, v in assessment_durations_dict.items():\n        if v != []:\n            assessment_durations_list.append(v[0])\n            \n    game_durations_list = []\n    for k, v in game_durations_dict.items():\n        if v!= []:\n            game_durations_list.append(v[0])\n    \n    '''if all_assessments != []:\n        for features_dict in all_assessments:\n            features_dict['mean_first_game_accuracy'] = 0\n            features_dict['std_first_game_accuracy'] = 0\n            if game_accuracies_list != []:\n                features_dict['mean_first_game_accuracy'] = np.mean(game_accuracies_list)\n                features_dict['std_first_game_accuracy'] = np.std(game_accuracies_list)\n            features_dict['mean_first_game_duration'] = 0\n            features_dict['std_first_game_duration'] = 0\n            if game_durations_list != []:\n                features_dict['mean_first_game_duration'] = np.mean(game_durations_list)\n                features_dict['std_first_game_duration'] = np.std(game_durations_list)\n            features_dict['first_game_accuracy_per_second'] = features_dict['mean_first_game_accuracy'] / features_dict['mean_first_game_duration'] if features_dict['mean_first_game_duration'] != 0 else 0\n            \n            features_dict['mean_first_assessment_duration'] = 0\n            if assessment_durations_list != []:\n                features_dict['mean_first_assessment_duration'] = np.mean(assessment_durations_list)\n                features_dict['std_first_assessment_duration'] = np.std(assessment_durations_list) if np.std(assessment_durations_list) != np.nan else 0\n            features_dict['first_game_accuracy*first_assessment_duration'] = features_dict['mean_first_game_accuracy'] * features_dict['mean_first_assessment_duration']\n            features_dict['first_assessment_duration/first_game_accuracy'] = features_dict['mean_first_assessment_duration'] / features_dict['mean_first_game_accuracy'] if features_dict['mean_first_game_accuracy'] != 0 else 0\n            features_dict['first_game_accuracy + first_assessment_duration / 60'] = features_dict['mean_first_game_accuracy'] + (features_dict['mean_first_assessment_duration']/60)\n            features_dict['first_game_accuracy + 1/(first_assessment_duration / 60)'] = features_dict['mean_first_game_accuracy'] + 1/(features_dict['mean_first_assessment_duration']/60) if (features_dict['mean_first_assessment_duration']/60) != 0 else 0'''\n    \n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_train_and_test(train, test):\n    '''compiled_train = []\n    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort = False), position=0):\n        compiled_train += get_data(user_sample)\n    reduce_train = pd.DataFrame(compiled_train)\n    \n    title_value_counts_dict = {}\n    for title in assessment_titles:\n        title_df = reduce_train[reduce_train['session_title'] == activities_map[title]]\n        title_value_counts = title_df['accuracy_group'].value_counts(normalize=True)\n        title_value_counts_dict[title] = title_value_counts'''\n    \n    compiled_train = []\n    compiled_test_X = []\n    compiled_test = []\n    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort = False), position=0):\n        compiled_train += get_data(user_sample)#, title_value_counts_dict=title_value_counts_dict)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), position=0):\n        compiled_test_X += get_data(user_sample)#, title_value_counts_dict=title_value_counts_dict)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), position=0):\n        test_data = get_data(user_sample, test_set=True)#, title_value_counts_dict=title_value_counts_dict)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test_X = pd.DataFrame(compiled_test_X)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    \n    return reduce_train, reduce_test_X, reduce_test, categoricals","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tranform function to get the train and test set\nreduce_train, reduce_test_X, reduce_test, categoricals = get_train_and_test(train, test)\nprint(reduce_train.shape, reduce_test_X.shape, reduce_test.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train_save = reduce_train.copy()\nreduce_test_save = reduce_test.copy()\nreduce_test_X_save = reduce_test_X.copy()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train = reduce_train_save.copy()\nreduce_test = reduce_test_save.copy()\nreduce_test_X = reduce_test_X_save.copy()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_title_stats(df):\n    unique_titles = df['session_title'].unique()\n\n    title_stats_df = pd.DataFrame()\n    title_stats_df['session_title'] = unique_titles\n    \n    '''# value counts\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['value_count_{}'.format(j)] = 0\n\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['value_count_{}'.format(j)].iloc[i] = title_val_counts[j]'''\n    \n    '''# class weights\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        title_class_weights = [title_val_counts.max() / title_val_counts[i] for i in range(len(title_val_counts))]\n        for j in range(len(title_class_weights)):\n            title_stats_df['class_weight_{}'.format(j)] = 0\n\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        title_class_weights = [title_val_counts.max() / title_val_counts[i] for i in range(len(title_val_counts))]\n        for j in range(len(title_class_weights)):\n            title_stats_df['class_weight_{}'.format(j)].iloc[i] = title_class_weights[j]'''\n            \n    # accuracy group order\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['accuracy_group_order_{}'.format(j)] = 0\n\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['accuracy_group_order_{}'.format(j)].iloc[i] = title_val_counts.index[j]\n    \n    '''# frequency order\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['frequency_order_{}'.format(j)] = 0\n\n    for i in range(len(title_stats_df)):\n        title = title_stats_df['session_title'][i]\n        title_val_counts = df[df['session_title'] == title]['accuracy_group'].value_counts(normalize=True)\n        for j in range(len(title_val_counts)):\n            title_stats_df['frequency_order_{}'.format(j)].iloc[i] = title_val_counts.iloc[j]'''\n    \n    return title_stats_df","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def secondary_feature_engineering(df):\n    df['accumulated_total_attempts'] = df['accumulated_correct_attempts'] + df['accumulated_uncorrect_attempts']\n    df['accumulated_game_total_attempts'] = df['accumulated_game_correct_attempts'] + df['accumulated_game_incorrect_attempts']\n    df['accumulated_assessment_total_attempts'] = df['accumulated_assessment_correct_attempts'] + df['accumulated_assessment_incorrect_attempts']\n    \n    #\n    '''df['something_accuracy'] = df['accumulated_correct_attempts'] / df['accumulated_total_attempts']\n    df['something_accuracy'] = df['something_accuracy'].fillna(0)'''\n    '''df['something_accuracy_group'] = df['something_accuracy'].apply(categorise_accuracy_group)\n    df['game_accuracy'] = df['accumulated_game_correct_attempts'] / df['accumulated_game_total_attempts']\n    df['game_accuracy'] = df['game_accuracy'].fillna(0)\n    df['game_accuracy_group'] = df['game_accuracy'].apply(categorise_accuracy_group)\n    df['assessment_accuracy'] = df['accumulated_assessment_correct_attempts'] / df['accumulated_assessment_total_attempts']\n    df['assessment_accuracy'] = df['assessment_accuracy'].fillna(0)\n    df['assessment_accuracy_group'] = df['assessment_accuracy'].apply(categorise_accuracy_group)\n    df['accuracy_user_score'] = df[['something_accuracy', 'game_accuracy', 'assessment_accuracy']].sum(axis=1)\n    df['accuracy_group_user_score'] = df[['something_accuracy_group', 'game_accuracy_group', 'assessment_accuracy_group']].sum(axis=1)'''\n    \n    # 2\n    df['user_score'] = df[['mean_game_accuracy2', 'accumulated_accuracy']].sum(axis=1)\n    df['game_accuracy+mean_previous_accuracy'] = df[['mean_game_accuracy2', 'mean_previous_accuracy']].sum(axis=1)\n    df['game_accuracy+last_previous_accuracy'] = df[['mean_game_accuracy2', 'last_accuracy']].sum(axis=1)\n    \n    # attempts\n    '''df['first_correct_attempts'] = df.groupby('installation_id')['accumulated_correct_attempts'].transform('first')\n    df['first_uncorrect_attempts'] = df.groupby('installation_id')['accumulated_uncorrect_attempts'].transform('first')\n    df['first_total_attempts'] = df.groupby('installation_id')['accumulated_total_attempts'].transform('first')\n    df['first_idk_accuracy'] = df['first_correct_attempts'] / df['first_total_attempts']\n    df['first_idk_accuracy'] = df['first_idk_accuracy'].fillna(0)'''\n        \n    # assessment attempts\n    '''df['first_assessment_correct_attempts'] = df.groupby('installation_id')['accumulated_assessment_correct_attempts'].transform('first')\n    df['first_assessment_incorrect_attempts'] = df.groupby('installation_id')['accumulated_assessment_incorrect_attempts'].transform('first')\n    df['first_assessment_total_attempts'] = df.groupby('installation_id')['accumulated_assessment_total_attempts'].transform('first')\n    df['first_assessment_accuracy'] = df['first_assessment_correct_attempts'] / df['first_assessment_total_attempts']\n    df['first_assessment_accuracy'] = df['first_assessment_accuracy'].fillna(0)'''\n    \n    return df","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function that creates more features\ndef preprocess(reduce_train, reduce_test, reduce_test_X, categoricals):\n    \n    # new val scheme\n    '''reduce_train = reduce_train.sort_values(['installation_id', 'accumulated_actions']).reset_index(drop=True)\n    unique_titles = reduce_train['session_title'].unique()\n    last_title_dfs_list = []\n    for i in range(len(unique_titles)):\n        title_df = reduce_train[reduce_train['session_title'] == unique_titles[i]]\n        title_last_df = title_df.groupby('installation_id').last()\n        last_title_dfs_list.append(title_last_df)\n\n    reduce_train = pd.concat(last_title_dfs_list).reset_index()[reduce_train.columns].sort_values(['installation_id', 'accumulated_actions'])'''\n    \n    # title stats feature engineering\n    '''train_title_stats_df = create_title_stats(reduce_train)\n    reduce_train = reduce_train.merge(train_title_stats_df, on=['session_title'])\n    reduce_train = reduce_train.sort_values(['installation_id', 'accumulated_actions'])\n    reduce_test = reduce_test.merge(train_title_stats_df, on=['session_title'])\n    reduce_test_X = reduce_test_X.merge(train_title_stats_df, on=['session_title'])'''\n    \n    for df in [reduce_train, reduce_test, reduce_test_X]:\n        #df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        #df['first_duration_mean'] = df.groupby(['installation_id'])['assessment_duration_mean'].transform('first')\n        '''df['mean_assessment_duration_mean'] = df.groupby(['installation_id'])['assessment_duration_mean'].transform('mean')\n        df['std_assessment_duration_mean'] = df.groupby(['installation_id'])['assessment_duration_mean'].transform('std')\n        df['std_assessment_duration_mean'] = df['std_assessment_duration_mean'].fillna(0)'''\n        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        #df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        #df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n        df = secondary_feature_engineering(df)\n        '''df['first_game_total_attempts'] = df['first_game_correct_attempts'] + df['first_game_incorrect_attempts']\n        df['first_game_accuracy'] = df['first_game_correct_attempts'] / df['first_game_total_attempts']\n        df['first_game_accuracy'] = df['first_game_accuracy'].fillna(0)'''\n        '''df['first_user_score'] = df.groupby('installation_id')['user_score'].transform('first')\n        df['mean_game_accuracy'] = df.groupby('installation_id')['game_accuracy'].transform('mean')\n        #df['mean_user_score'] = df.groupby('installation_id')['user_score'].transform('mean')\n        df['std_game_accuracy'] = df.groupby('installation_id')['game_accuracy'].transform('std')\n        df['std_game_accuracy'] = df['std_game_accuracy'].fillna(0)'''\n        #df['std_user_score'] = df.groupby('installation_id')['user_score'].transform('std')\n        #df['std_user_score'] = df['std_user_score'].fillna(0)\n        \n        #\n        '''for title in game_titles:\n            df[title+'_total_attempts'] = df[title+'_correct_attempts'] + df[title+'_incorrect_attempts']\n            df[title+'_accuracy'] = df[title+'_correct_attempts'] / df[title+'_total_attempts']\n            df[title+'_accuracy'] = df[title+'_accuracy'].fillna(0)\n            df[title+'_accuracy_group'] = df[title+'_accuracy'].apply(categorise_accuracy_group)'''\n        \n    '''for cat in categoricals:\n        agg = reduce_train.groupby(cat)['accuracy_group'].agg({'mean', 'std'})\n        agg.columns = ['std_accuracy_group_per_{}'.format(cat), 'mean_accuracy_group_per_{}'.format(cat)]\n        reduce_train = reduce_train.merge(agg, on=['{}'.format(cat)])\n        reduce_test = reduce_test.merge(agg, on=['{}'.format(cat)])\n        reduce_test_X = reduce_test_X.merge(agg, on=['{}'.format(cat)])'''    \n    \n    reduce_train = reduce_train.sort_values(['installation_id', 'accumulated_actions']).reset_index(drop=True)\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = features.drop(['accuracy_group', 'installation_id', 'accuracy', 'is_first_session', 'is_last_session'])\n    features = sorted([str(col) for col in features if 'pseudo' not in str(col)])\n   \n    return reduce_train, reduce_test, reduce_test_X, features","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# call feature engineering function\n#reduce_test = pd.concat([reduce_test_X, reduce_test]).sort_values(['installation_id', 'accumulated_actions'])\nreduce_train, reduce_test, reduce_test_X, features = preprocess(reduce_train, reduce_test, reduce_test_X, categoricals)\nreduce_test_X['is_last_session'] = 0\n#reduce_test = pd.concat([reduce_test, reduce_test_X]).sort_values(['installation_id', 'accumulated_actions']).reset_index(drop=True)\nreduce_train = pd.concat([reduce_train, reduce_test_X]).sort_values(['installation_id', 'accumulated_actions']).reset_index(drop=True)\n#reduce_test = reduce_test.groupby(['installation_id']).last().reset_index()\nprint(reduce_train.shape, reduce_test.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train.columns = [str(col) for col in reduce_train.columns]\nreduce_test.columns = [str(col) for col in reduce_test.columns]","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train[[col for col in reduce_train.columns if reduce_train[col].isna().any() == True]] = reduce_train[[col for col in reduce_train.columns if reduce_train[col].isna().any() == True]].fillna(0)\nreduce_test[[col for col in reduce_test.columns if reduce_test[col].isna().any() == True]] = reduce_test[[col for col in reduce_test.columns if reduce_test[col].isna().any() == True]].fillna(0)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counter = 0\nto_remove = []\nfor feat_a in features:\n    for feat_b in features:\n        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n            if c >= 1:\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = sorted([col for col in features if col not in to_remove])\nprint(len(features))","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = ['solved_before', 'attempted_before', 'is_last_session', 'is_first_session', 'session_title']","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train.head(20)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduce_train['prac_accuracy'] = reduce_train['accuracy'] * 0.6 + reduce_train['pseudo_accuracy'] * 0.4\nreduce_train['target_accuracy'] = reduce_train['prac_accuracy'].apply(lambda x: 1-1/4**(x))\nreduce_train['target_accuracy'] = reduce_train['target_accuracy'] * (3 / reduce_train['target_accuracy'].max())\nprint(reduce_train['target_accuracy'].describe())\nsns.distplot(reduce_train['target_accuracy'])\nplt.show()\nplt.clf()\noptR = OptimizedRounder()\noptR.fit(reduce_train['target_accuracy'], reduce_train['accuracy_group'])\ncoefficients = optR.coefficients()\nopt_preds = optR.predict(reduce_train['target_accuracy'], coefficients)\nprint('kappa score:', cohen_kappa_score(reduce_train['accuracy_group'], opt_preds, weights='quadratic'))","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols_to_drop = ['is_first_session', 'is_last_session', 'user_score', 'game_accuracy+mean_previous_accuracy', 'game_accuracy+last_previous_accuracy']\nfeatures = sorted([col for col in features if col not in cols_to_drop])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_features = ['hour', 'day', 'dayofweek', 'week', 'month', 'year']\ncategoricals = ['session_title'] + [col for col in features if col in time_features]\ncategoricals","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Installation_id as a feature","metadata":{}},{"cell_type":"code","source":"categoricals = categoricals + ['installation_id']\ncategoricals","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = sorted(features + ['installation_id'])\nlen(features)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_no_previous_records_df = reduce_train[(reduce_train['is_last_session'] == 1) & (reduce_train['is_first_session'] == 1)]\ntemp_reduce_test_X = reduce_train[reduce_train['installation_id'].isin(reduce_test_X['installation_id'])]\ntemp_reduce_train = reduce_train[~reduce_train['installation_id'].isin(reduce_test_X['installation_id'])]\ntrain_records_df = temp_reduce_train[~temp_reduce_train['installation_id'].isin(train_no_previous_records_df['installation_id'])]\n\nunique_ids_df = pd.DataFrame()\nunique_ids_df['installation_id'] = train_records_df['installation_id'].unique()\n\ntrain_records_df = pd.concat([train_records_df, temp_reduce_test_X]).sort_values(['installation_id', 'accumulated_actions']).reset_index(drop=True)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def catboost_plot_importances(features, model, n_features=30):\n    df = pd.DataFrame()\n    df['features'] = features\n    df['importance'] = model.get_feature_importance()\n    df = df.sort_values('importance', ascending=False)\n    sns.barplot(df['importance'][:n_features], df['features'][:n_features])\n    plt.show()\n    plt.clf()\n    \n    return df[df['importance'] > 0]","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ids_df = unique_ids_df.copy()\nval_ids_list = []\nmodels_list1 = []\nfor i in range(5):\n    print('------ Fold {} starting -------'.format(i))\n    val_ids_df = ids_df.sample(frac=1/(5-i), random_state=42)\n    val_ids_list += val_ids_df['installation_id'].unique().tolist()\n    ids_df = ids_df[~ids_df['installation_id'].isin(val_ids_list)]\n    \n    train_data = train_records_df[~train_records_df['installation_id'].isin(val_ids_df['installation_id'])]\n    val_data = train_records_df[train_records_df['installation_id'].isin(val_ids_df['installation_id'])]\n    val_last = val_data[val_data['is_last_session'] == 1]\n    val_not_last = val_data[val_data['is_last_session'] == 0]\n    train_data = pd.concat([train_data, val_not_last]).reset_index(drop=True)\n    val_data = val_last.copy()\n    \n    x_train = train_data[features]\n    y_train = train_data['target_accuracy']\n    \n    x_val = val_data[features]\n    y_val = val_data['target_accuracy']\n    y_val_acc_group = val_data['accuracy_group']\n    \n    train_pool = Pool(x_train, y_train, cat_features=categoricals)\n    val_pool = Pool(x_val, y_val, cat_features=categoricals)\n    \n    model = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', iterations=1e6, learning_rate=0.01,\n                                   random_seed=42, use_best_model=True, depth=8, has_time=True)\n    \n    model.fit(train_pool, eval_set=val_pool, verbose=100, early_stopping_rounds=300)\n    models_list1.append(model)\n    preds = model.predict(x_val)\n    top_features = catboost_plot_importances(features, model)\n    optR = OptimizedRounder()\n    optR.fit(preds.reshape(-1,), y_val_acc_group, method='Powell')\n    coefficients = optR.coefficients()\n    print('optimized coefficients:', coefficients)\n    opt_preds = optR.predict(preds.reshape(-1, ), coefficients)\n    print('Fold kappa score:', cohen_kappa_score(y_val_acc_group, opt_preds, weights='quadratic'))   \n    print('------ Fold {} finished ------'.format(i))","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"previous_records_df = reduce_test[reduce_test['installation_id'].isin(reduce_test_X['installation_id'])]\nno_records_df = reduce_test[~reduce_test['installation_id'].isin(reduce_test_X['installation_id'])]\nprint(previous_records_df.shape, no_records_df.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_list = []\nrank_preds_list = []\nfor model in models_list1:\n    preds = model.predict(train_records_df[features])\n    preds_list.append(preds)\n    preds = rankdata(preds)\n    rank_preds_list.append(preds)\ntrain_records_df['raw_preds'] = np.mean(preds_list, axis=0)\ntrain_records_df['rank'] = np.mean(rank_preds_list, axis=0)\nsns.distplot(train_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(train_records_df['rank'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_list = []\nrank_preds_list = []\nfor model in models_list1:\n    preds = model.predict(previous_records_df[features])\n    preds_list.append(preds)\n    preds = rankdata(preds)\n    rank_preds_list.append(preds)\nprevious_records_df['raw_preds'] = np.mean(preds_list, axis=0)\nprevious_records_df['rank'] = np.mean(rank_preds_list, axis=0)\nsns.distplot(previous_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(previous_records_df['rank'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(train_records_df['raw_preds'])\nsns.distplot(previous_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"last_df = train_records_df[train_records_df['is_last_session'] == 1]\nprint(last_df.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(last_df['raw_preds'])\nsns.distplot(previous_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_titles = previous_records_df['session_title'].unique() ","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for title in unique_titles:\n    train_title_df = last_df[last_df['session_title'] == title]\n    test_title_df = previous_records_df[previous_records_df['session_title'] == title]\n    \n    sns.distplot(train_title_df['raw_preds'])\n    sns.distplot(test_title_df['raw_preds'])\n    plt.show()\n    plt.clf()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp_test_X = train_records_df[train_records_df['installation_id'].isin(reduce_test_X['installation_id'])]\nprint(temp_test_X.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(temp_test_X['raw_preds'])\nsns.distplot(previous_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for title in unique_titles:\n    train_title_df = temp_test_X[temp_test_X['session_title'] == title]\n    test_title_df = previous_records_df[previous_records_df['session_title'] == title]\n    \n    sns.distplot(train_title_df['raw_preds'])\n    sns.distplot(test_title_df['raw_preds'])\n    plt.show()\n    plt.clf()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concat_df = pd.concat([last_df, temp_test_X])\nprint(concat_df.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(concat_df['raw_preds'])\nsns.distplot(previous_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for title in unique_titles:\n    train_title_df = concat_df[concat_df['session_title'] == title]\n    test_title_df = previous_records_df[previous_records_df['session_title'] == title]\n    \n    sns.distplot(train_title_df['raw_preds'])\n    sns.distplot(test_title_df['raw_preds'])\n    plt.show()\n    plt.clf()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"title_ratio = previous_records_df['session_title'].value_counts(normalize=True)\nacc_group_dist = {0:0, 1:0, 2:0, 3:0}\nfor title in unique_titles:\n    title_df = temp_test_X[temp_test_X['session_title'] == title]\n    title_acc_group_dist = title_df['accuracy_group'].value_counts(normalize=True)\n    for i in range(len(title_acc_group_dist)):\n        acc_group_dist[i] += title_acc_group_dist[i] * title_ratio[title]\nacc_group_dist","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acum = 0\nbound = {}\nfor i in range(3):\n    acum += acc_group_dist[i]\n    bound[i] = np.percentile(previous_records_df['rank'], acum * 100)\nprint(bound)\n\nprevious_records_df['dist_preds'] = np.array(list(map(classify, previous_records_df['rank']))).astype('int')","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"previous_records_df['dist_preds'].value_counts(normalize=True)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# No installation_id as a feature","metadata":{}},{"cell_type":"code","source":"time_features = ['hour', 'day', 'dayofweek', 'week', 'month', 'year']\ncategoricals = ['session_title'] + [col for col in features if col in time_features]\ncategoricals","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features.remove('installation_id')\nfeatures = sorted(features)\nlen(features)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_catboost_regression(reduce_train, usefull_features):\n    kf = GroupKFold(n_splits=5)#, shuffle=True, random_state=42)\n    target = 'target_accuracy'\n    models_list = []\n    top_features_list = []\n    oof = np.zeros(len(reduce_train))\n    for fold, (train_idx, val_idx) in enumerate(kf.split(reduce_train, reduce_train[target], reduce_train['installation_id'])):\n        print('Fold {}'.format(fold + 1))\n        x_train = reduce_train[usefull_features].iloc[train_idx]\n        y_train = reduce_train[target].iloc[train_idx]\n        val_data = reduce_train.iloc[val_idx]\n        val_data = val_data[val_data['is_first_session'] == 1]\n        x_val = val_data[usefull_features]\n        y_val = val_data[target]\n        y_acc_group = val_data['accuracy_group']\n        x_val_for_oof = reduce_train[usefull_features].iloc[val_idx]\n        y_val_for_oof = reduce_train[target].iloc[val_idx]\n        y_acc_group_for_oof = reduce_train['accuracy_group'].iloc[val_idx]\n        \n        train_pool = Pool(x_train, y_train, cat_features=categoricals)\n        val_pool = Pool(x_val, y_val, cat_features=categoricals)\n\n        model = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', iterations=1e6, learning_rate=0.01,\n                                   random_seed=42, use_best_model=True, depth=8, has_time=True)\n        model.fit(train_pool, eval_set=val_pool, verbose=100, early_stopping_rounds=300)\n        preds = model.predict(x_val_for_oof)\n        models_list.append(model)\n        top_features = catboost_plot_importances(usefull_features, model)\n        top_features_list.append(top_features)\n        oof[val_idx] = preds\n        optR = OptimizedRounder()\n        optR.fit(preds.reshape(-1,), y_acc_group_for_oof, method='Powell')\n        coefficients = optR.coefficients()\n        print('optimized coefficients:', coefficients)\n        opt_preds = optR.predict(preds.reshape(-1, ), coefficients)\n        print('Fold kappa score:', cohen_kappa_score(y_acc_group_for_oof, opt_preds, weights='quadratic'))\n        \n    \n    optR = OptimizedRounder()\n    optR.fit(oof.reshape(-1,), reduce_train['accuracy_group'])\n    coefficients = optR.coefficients()\n    print('optimized coefficients:', coefficients)\n    opt_preds = optR.predict(oof.reshape(-1,), coefficients)\n    print(pd.Series(opt_preds).value_counts(normalize=True))\n    print('oof kappa score:', cohen_kappa_score(reduce_train['accuracy_group'], opt_preds, weights='quadratic'))\n    print('\\n')\n        \n    return models_list, top_features_list, coefficients, optR, oof","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models_list2, top_features_list, coefficients, optR, oof = run_catboost_regression(reduce_train, features)\nreduce_train['oof'] = oof","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_list = []\nrank_preds_list = []\nfor model in models_list2:\n    preds = model.predict(reduce_train[features])\n    preds_list.append(preds)\n    preds = rankdata(preds)\n    rank_preds_list.append(preds)\nreduce_train['raw_preds'] = np.mean(preds_list, axis=0)\nreduce_train['rank'] = np.mean(rank_preds_list, axis=0)\nsns.distplot(reduce_train['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(reduce_train['rank'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_list = []\nrank_preds_list = []\nfor model in models_list2:\n    preds = model.predict(no_records_df[features])\n    preds_list.append(preds)\n    preds = rankdata(preds)\n    rank_preds_list.append(preds)\nno_records_df['raw_preds'] = np.mean(preds_list, axis=0)\nno_records_df['rank'] = np.mean(rank_preds_list, axis=0)\nsns.distplot(no_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(no_records_df['rank'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(reduce_train['raw_preds'])\nsns.distplot(no_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_df = reduce_train[reduce_train['is_first_session'] == 1]\nprint(first_df.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(first_df['raw_preds'])\nsns.distplot(no_records_df['raw_preds'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(first_df['rank'])\n# not really sure what this graph means","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.distplot(no_records_df['rank'])","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_titles = no_records_df['session_title'].unique()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for title in unique_titles:\n    train_title_df = first_df[first_df['session_title'] == title]\n    test_title_df = no_records_df[no_records_df['session_title'] == title]\n    \n    sns.distplot(train_title_df['raw_preds'])\n    sns.distplot(test_title_df['raw_preds'])\n    plt.show()\n    plt.clf()","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"title_ratio = no_records_df['session_title'].value_counts(normalize=True)\nacc_group_dist = {0:0, 1:0, 2:0, 3:0}\nfor title in unique_titles:\n    title_df = first_df[first_df['session_title'] == title]\n    title_acc_group_dist = title_df['accuracy_group'].value_counts(normalize=True)\n    for i in range(len(title_acc_group_dist)):\n        acc_group_dist[i] += title_acc_group_dist[i] * title_ratio[title]\nacc_group_dist","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acum = 0\nbound = {}\nfor i in range(3):\n    acum += acc_group_dist[i]\n    bound[i] = np.percentile(no_records_df['rank'], acum * 100)\nprint(bound)\n\nno_records_df['dist_preds'] = np.array(list(map(classify, no_records_df['rank']))).astype('int')","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"no_records_df['dist_preds'].value_counts(normalize=True)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.concat([previous_records_df, no_records_df]).reset_index(drop=True)\nprint(test_df.shape)","metadata":{"trusted":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = sample_submission[['installation_id']].merge(test_df[['installation_id', 'dist_preds']], on=['installation_id'])\nsubmission.columns = ['installation_id', 'accuracy_group']\nsubmission.to_csv('submission.csv', index=False)\nsubmission['accuracy_group'].value_counts(normalize=True)","metadata":{"trusted":false},"outputs":[],"execution_count":null}]}