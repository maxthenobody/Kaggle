{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Original notebook: https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer (version 12)","metadata":{}},{"cell_type":"code","source":"import sys\n#sys.path.insert(0, \"../input/transformers/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n#import tensorflow_hub as hub\nimport tensorflow as tf\n#import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\nimport seaborn as sns\n\nnp.set_printoptions(suppress=True)\npd.set_option('display.max_columns', 500)\nprint(tf.__version__)\n\nimport random\nrandom.seed(42)\nnp.random.seed(42)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 1. Read data and tokenizer\n\nRead tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)","metadata":{}},{"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\n\n# BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n# tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n\nBERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 384\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Additional features","metadata":{}},{"cell_type":"code","source":"concat_df = pd.concat([df_train, df_test])\nconcat_df['question_user==answer_user'] = (concat_df['question_user_name'] == concat_df['answer_user_name']).astype('int')\nconcat_df['title_length'] = concat_df['question_title'].apply(lambda x: len(x))\nconcat_df['question_length'] = concat_df['question_body'].apply(lambda x: len(x))\nconcat_df['answer_length'] = concat_df['answer'].apply(lambda x: len(x))\nconcat_df['title_split'] = concat_df['question_title'].str.split(' ')\nconcat_df['question_split'] = concat_df['question_body'].str.split(' ')\nconcat_df['answer_split'] = concat_df['answer'].str.split(' ')\nconcat_df['title_split_length'] = concat_df['title_split'].apply(lambda x: len(x))\nconcat_df['question_split_length'] = concat_df['question_split'].apply(lambda x: len(x))\nconcat_df['answer_split_length'] = concat_df['answer_split'].apply(lambda x: len(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_unique_words = lambda x: pd.Series(x).nunique()\nconcat_df['title_num_unique_words'] = concat_df['title_split'].apply(num_unique_words)\nconcat_df['question_num_unique_words'] = concat_df['question_split'].apply(num_unique_words)\nconcat_df['answer_num_unique_words'] = concat_df['answer_split'].apply(num_unique_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concat_df['title_word_is_in_question'] = 0\nconcat_df['title_word_num_appearance_in_question'] = 0\nconcat_df['title_word_is_in_answer'] = 0\nconcat_df['title_word_num_appearance_in_answer'] = 0\nconcat_df['question_word_is_in_answer'] = 0\nconcat_df['question_word_num_appearance_in_answer'] = 0\nfor i, row in tqdm(concat_df[['title_split', 'question_split', 'answer_split']].iterrows(), position=0):\n    title_row = row['title_split']\n    question_row = row['question_split']\n    answer_row = row['answer_split']\n    title_question_count = 0\n    title_answer_count = 0\n    question_answer_count = 0\n  \n  # title - question & title - answer\n    for word in title_row:\n        if word in question_row:\n            title_question_count += 1\n        if word in answer_row:\n            title_answer_count += 1\n        # question - answer\n        for word in question_row:\n            if word in answer_row:\n                question_answer_count += 1\n\n  # calc \n    if title_question_count > 0:\n        concat_df['title_word_is_in_question'][i] = 1\n        concat_df['title_word_num_appearance_in_question'][i] = title_question_count\n    if title_answer_count > 0:\n        concat_df['title_word_is_in_answer'][i] = 1\n        concat_df['title_word_num_appearance_in_answer'][i] = title_answer_count\n    if question_answer_count > 0:\n        concat_df['question_word_is_in_answer'][i] = 1\n        concat_df['question_word_num_appearance_in_answer'][i] = question_answer_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# stopwords\n#!pip install nltk\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words('english'))\n\nnum_stopwords = lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords])\nconcat_df['title_num_stopwords'] = concat_df['question_title'].apply(num_stopwords)\nconcat_df['question_num_stopwords'] = concat_df['question_body'].apply(num_stopwords)\nconcat_df['answer_num_stopwords'] = concat_df['answer'].apply(num_stopwords)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# punctuations\nimport string\nnum_punctuations = lambda x: len([c for c in str(x) if c in string.punctuation])\nconcat_df['title_num_punctuations'] = concat_df['question_title'].apply(num_punctuations)\nconcat_df['question_num_punctuations'] = concat_df['question_body'].apply(num_punctuations)\nconcat_df['answer_num_punctuations'] = concat_df['answer'].apply(num_punctuations)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# upper case words\nnum_uppers = lambda x: len([w for w in str(x).split() if w.isupper()])\nconcat_df['title_num_uppers'] = concat_df['question_title'].apply(num_uppers)\nconcat_df['question_num_uppers'] = concat_df['question_body'].apply(num_uppers)\nconcat_df['answer_num_uppers'] = concat_df['answer'].apply(num_uppers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# titles\nnum_titles = lambda x: len([s for s in str(x).split('. ') if s.istitle()])\nconcat_df['title_num_titles'] = concat_df['question_title'].apply(num_titles)\nconcat_df['question_num_titles'] = concat_df['question_body'].apply(num_titles)\nconcat_df['answer_num_titles'] = concat_df['answer'].apply(num_titles)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nnumeric_cols = ['title_length', 'question_length', 'answer_length', 'title_word_num_appearance_in_question', 'title_word_num_appearance_in_answer',\n                'question_word_num_appearance_in_answer', 'title_num_stopwords', 'question_num_stopwords', 'answer_num_stopwords', 'title_num_punctuations',\n                'question_num_punctuations', 'answer_num_punctuations', 'title_num_uppers', 'question_num_uppers', 'answer_num_uppers', 'title_num_titles', 'question_num_titles',\n                'answer_num_titles', 'title_num_unique_words', 'question_num_unique_words', 'answer_num_unique_words']\nstd_scaler = MinMaxScaler(feature_range=(-1,1))\nconcat_df[numeric_cols] = std_scaler.fit_transform(concat_df[numeric_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sentiment analysis\nfrom textblob import TextBlob\n\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\nconcat_df['title_polarity'] = concat_df['question_title'].apply(pol)\nconcat_df['title_subjectivity'] = concat_df['question_title'].apply(sub)\nconcat_df['question_polarity'] = concat_df['question_body'].apply(pol)\nconcat_df['question_subjectivity'] = concat_df['question_body'].apply(sub)\nconcat_df['answer_polarity'] = concat_df['answer'].apply(pol)\nconcat_df['answer_subjectivity'] = concat_df['answer'].apply(sub)\n\nsentiment_features = ['title_polarity', 'title_subjectivity', 'question_polarity', 'question_subjectivity', 'answer_polarity', 'answer_subjectivity']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concat_dummies_df = pd.get_dummies(concat_df, columns=['category', 'host'])\nprint(concat_dummies_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_cols = [col for col in concat_dummies_df.columns if ('host' in col) | ('category' in col)]\nprint(len(cat_cols))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dummies_df = concat_dummies_df.head(len(df_train))\ntest_dummies_df = concat_dummies_df.tail(len(df_test))\nprint(train_dummies_df.shape, test_dummies_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"additional_features = ['question_user==answer_user', 'title_word_is_in_question', 'title_word_is_in_answer', 'question_word_is_in_answer'] + cat_cols + sentiment_features\\\n+ numeric_cols\nadditional_features = sorted(additional_features)\nprint(len(additional_features))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bert & DNN","metadata":{}},{"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr_ignore_nan(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        rho_val2 = compute_spearmanr_ignore_nan(self.valid_outputs, self.valid_predictions[-1])\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        print('\\nvalidation rho2:', rho_val2)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        if epoch > 1:\n            self.test_predictions.append(\n                self.model.predict(self.test_inputs, batch_size=self.batch_size)\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\nq_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\nq_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\nconfig = BertConfig() # print(config) to see settings\nconfig.output_hidden_states = False # Set to True to obtain hidden states\n# caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n\n# normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n# pretrained model has been downloaded manually and uploaded to kaggle. \nbert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n\n# if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\nout = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)\n\n#hmm = tf.keras.layers.GlobalAveragePooling1D()(out[0])\n\nmodel = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn], outputs=out)'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n\nvalid_preds = []\ntest_preds = []\nbatch_size = 6\nhistories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    if fold in [0, 2, 4]:\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n\n        # additional features\n        train_data = train_dummies_df.iloc[train_idx]\n        val_data = train_dummies_df.iloc[valid_idx]\n\n        train_add_features = train_data[additional_features].values\n        val_add_features = val_data[additional_features].values\n        test_add_features = test_dummies_df[additional_features].values\n\n        # model\n        K.clear_session()\n\n        # bert layer inputs\n        q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n        a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n        q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n        a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n        q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n        a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n        # additional features inputs\n        add_f_in = tf.keras.layers.Input(shape=(train_add_features.shape[1],))\n\n        config = BertConfig() # print(config) to see settings\n        config.output_hidden_states = True # Set to True to obtain hidden states\n        # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n\n        # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n        # pretrained model has been downloaded manually and uploaded to kaggle. \n        bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n\n        # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n        q_hidden = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[-1]\n        a_hidden = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[-1]\n\n        q = tf.keras.layers.Average()([q_hidden[-i-1] for i in range(4)])\n        a = tf.keras.layers.Average()([a_hidden[-i-1] for i in range(4)])\n\n        q = tf.keras.layers.GlobalAveragePooling1D()(q)\n        a = tf.keras.layers.GlobalAveragePooling1D()(a)\n\n        #x = tf.keras.layers.Dense(8, activation='relu')(add_f_in)\n\n        x = tf.keras.layers.Concatenate()([q, a])\n\n        x = tf.keras.layers.Dropout(0.2)(x)\n\n        #x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n        #x = tf.keras.layers.Dropout(0.25)(x)\n\n        x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n        model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, add_f_in], outputs=x)\n\n        custom_callback = CustomCallback(\n            valid_data=(valid_inputs + [val_add_features], valid_outputs), \n            test_data=test_inputs + [test_add_features],\n            batch_size=batch_size,\n            fold=None\n        )\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        display(model.summary())\n\n        # train\n        history = model.fit(train_inputs + [train_add_features], train_outputs, epochs=3, batch_size=batch_size, validation_data=(valid_inputs + [val_add_features], valid_outputs),\n                            callbacks=[custom_callback]\n                           )\n\n        model.save_weights('model{}.h5'.format(fold))\n\n        plt.plot(history.history['loss'])\n        plt.plot(history.history['val_loss'])\n        plt.show()\n        plt.clf()\n\n        valid_preds.append(model.predict(valid_inputs + [val_add_features]))\n        test_preds.append(model.predict(test_inputs + [test_add_features]))\n\n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)\n\n        histories.append(custom_callback)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6. Process and submit test predictions\n\nAverage fold predictions, then save as `submission.csv`","metadata":{}},{"cell_type":"code","source":"test_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}